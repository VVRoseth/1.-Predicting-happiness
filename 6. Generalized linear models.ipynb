{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Happiness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the clean and dummied dataset in\n",
    "colstep = pd.read_csv('./Datasets /STEPColombia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_school</th>\n",
       "      <th>owns_house</th>\n",
       "      <th>house_beds</th>\n",
       "      <th>house_kitchen</th>\n",
       "      <th>reported_social_status</th>\n",
       "      <th>got_pr_transf</th>\n",
       "      <th>part_in_training</th>\n",
       "      <th>life_satisfaction</th>\n",
       "      <th>offdays_ill</th>\n",
       "      <th>healthinsurance</th>\n",
       "      <th>...</th>\n",
       "      <th>labor_market_status_1.0</th>\n",
       "      <th>job_stable_1.0</th>\n",
       "      <th>highest_ISCED_PIAAC_1</th>\n",
       "      <th>highest_ISCED_PIAAC_2</th>\n",
       "      <th>highest_ISCED_PIAAC_3</th>\n",
       "      <th>highest_ISCED_PIAAC_5</th>\n",
       "      <th>highest_ISCED_PIAAC_6</th>\n",
       "      <th>highest_ISCED_PIAAC_7</th>\n",
       "      <th>highest_ISCED_PIAAC_8</th>\n",
       "      <th>dropout_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   in_school  owns_house  house_beds  house_kitchen  reported_social_status  \\\n",
       "0        0.0           2           3              1                       3   \n",
       "1        0.0           2           1              1                       3   \n",
       "2        0.0           2           2              1                       3   \n",
       "3        0.0           2           2              1                       3   \n",
       "4        0.0           1           1              1                       3   \n",
       "\n",
       "   got_pr_transf  part_in_training  life_satisfaction  offdays_ill  \\\n",
       "0              0               0.0                9.0          0.0   \n",
       "1              0               1.0                9.0          0.0   \n",
       "2              0               0.0                5.0          0.0   \n",
       "3              0               0.0                7.0          0.0   \n",
       "4              0               0.0                7.0          0.0   \n",
       "\n",
       "   healthinsurance  ...  labor_market_status_1.0  job_stable_1.0  \\\n",
       "0              1.0  ...                        0               0   \n",
       "1              1.0  ...                        1               1   \n",
       "2              1.0  ...                        0               0   \n",
       "3              1.0  ...                        0               0   \n",
       "4              1.0  ...                        1               1   \n",
       "\n",
       "   highest_ISCED_PIAAC_1  highest_ISCED_PIAAC_2  highest_ISCED_PIAAC_3  \\\n",
       "0                      0                      0                      1   \n",
       "1                      0                      0                      0   \n",
       "2                      1                      0                      0   \n",
       "3                      0                      0                      1   \n",
       "4                      0                      0                      1   \n",
       "\n",
       "   highest_ISCED_PIAAC_5  highest_ISCED_PIAAC_6  highest_ISCED_PIAAC_7  \\\n",
       "0                      0                      0                      0   \n",
       "1                      1                      0                      0   \n",
       "2                      0                      0                      0   \n",
       "3                      0                      0                      0   \n",
       "4                      0                      0                      0   \n",
       "\n",
       "   highest_ISCED_PIAAC_8  dropout_1.0  \n",
       "0                      0            0  \n",
       "1                      0            0  \n",
       "2                      0            1  \n",
       "3                      0            1  \n",
       "4                      0            0  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming this is the correct dataset \n",
    "colstep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2617, 114)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colstep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Poisson Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Identifying X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the X values. Excluding life satisfaction and other features that have dummies. \n",
    "X = colstep.drop(columns=['life_satisfaction','in_school', 'owns_house', 'house_beds',\n",
    "                          'house_kitchen','reported_social_status', 'got_pr_transf', \n",
    "                          'got_pu_transf', 'part_in_training','life_satisfaction', \n",
    "                          'offdays_ill', 'healthinsurance','speak_other_languaje', \n",
    "                          'lives_w_mother', 'lives_w_father','read_overall', \n",
    "                          'write_overall', 'numeracy_overall', 'supervise', \n",
    "                          'computer_use_overall', 'think_learn_work', 'autonomy_at_work',\n",
    "                          'repetitiveness_at_work', 'physical_demand_work', \n",
    "                          'has_children', 'hh_size', 'gender', 'has_spouse',\n",
    "                          'chronic_disease', 'shocks_bef_15', 'mother_tongue',\n",
    "                          'labor_market_status', 'job_stable', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vvroseth/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting y \n",
    "y = colstep['life_satisfaction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Fitting poisson regression on 10-scale Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vvroseth/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Instantiating and fiting the model using stats models  \n",
    "poi_reg = sm.GLM(y, X, \n",
    "                family=sm.families.Poisson(link = sm.families.links.log)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>life_satisfaction</td> <th>  No. Observations:  </th>  <td>  2617</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>        <th>  Df Residuals:      </th>  <td>  2537</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>         <td>Poisson</td>      <th>  Df Model:          </th>  <td>    79</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>          <td>log</td>        <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>        <th>  Log-Likelihood:    </th> <td> -5665.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 19 Mar 2020</td>  <th>  Deviance:          </th> <td>  1172.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>15:17:52</td>      <th>  Pearson chi2:      </th> <td>1.06e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>4</td>         <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                      <td>    2.0954</td> <td>    0.251</td> <td>    8.361</td> <td> 0.000</td> <td>    1.604</td> <td>    2.587</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ses_15</th>                     <td>    0.0143</td> <td>    0.004</td> <td>    3.274</td> <td> 0.001</td> <td>    0.006</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>extraversion_av</th>            <td>   -0.0004</td> <td>    0.011</td> <td>   -0.034</td> <td> 0.973</td> <td>   -0.022</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>conscientiousness_avg</th>      <td>   -0.0212</td> <td>    0.015</td> <td>   -1.406</td> <td> 0.160</td> <td>   -0.051</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>openness_av</th>                <td>    0.0110</td> <td>    0.015</td> <td>    0.728</td> <td> 0.466</td> <td>   -0.019</td> <td>    0.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>stability_av</th>               <td>    0.0424</td> <td>    0.010</td> <td>    4.041</td> <td> 0.000</td> <td>    0.022</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>agreeableness_av</th>           <td>    0.0105</td> <td>    0.013</td> <td>    0.794</td> <td> 0.427</td> <td>   -0.015</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grit_av</th>                    <td>    0.0105</td> <td>    0.013</td> <td>    0.820</td> <td> 0.412</td> <td>   -0.015</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>decision_av</th>                <td>   -0.0250</td> <td>    0.013</td> <td>   -1.907</td> <td> 0.056</td> <td>   -0.051</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hostile_av</th>                 <td>   -0.0331</td> <td>    0.012</td> <td>   -2.669</td> <td> 0.008</td> <td>   -0.057</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>risk</th>                       <td>   -0.0043</td> <td>    0.007</td> <td>   -0.621</td> <td> 0.535</td> <td>   -0.018</td> <td>    0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                        <td>   -0.0005</td> <td>    0.001</td> <td>   -0.634</td> <td> 0.526</td> <td>   -0.002</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BMI</th>                        <td>   -0.0008</td> <td>    0.002</td> <td>   -0.398</td> <td> 0.691</td> <td>   -0.004</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hour_earnings</th>              <td>-7.581e-08</td> <td> 7.05e-07</td> <td>   -0.107</td> <td> 0.914</td> <td>-1.46e-06</td> <td> 1.31e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total_hr_worked_week</th>       <td> -4.85e-05</td> <td>    0.000</td> <td>   -0.120</td> <td> 0.904</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC</th>        <td>    0.0131</td> <td>    0.022</td> <td>    0.603</td> <td> 0.547</td> <td>   -0.029</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>years_educ</th>                 <td>   -0.0074</td> <td>    0.008</td> <td>   -0.930</td> <td> 0.352</td> <td>   -0.023</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wealth_index</th>               <td>    0.0347</td> <td>    0.012</td> <td>    2.991</td> <td> 0.003</td> <td>    0.012</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>overqualified</th>              <td>   -0.0145</td> <td>    0.008</td> <td>   -1.879</td> <td> 0.060</td> <td>   -0.030</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dropout</th>                    <td>   -0.0059</td> <td>    0.010</td> <td>   -0.567</td> <td> 0.571</td> <td>   -0.026</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>in_school_1.0</th>              <td>    0.0255</td> <td>    0.026</td> <td>    0.983</td> <td> 0.326</td> <td>   -0.025</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>owns_house_2</th>               <td>   -0.0071</td> <td>    0.018</td> <td>   -0.404</td> <td> 0.686</td> <td>   -0.042</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>owns_house_3</th>               <td>   -0.0401</td> <td>    0.029</td> <td>   -1.381</td> <td> 0.167</td> <td>   -0.097</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>house_beds_2</th>               <td>   -0.0126</td> <td>    0.018</td> <td>   -0.718</td> <td> 0.473</td> <td>   -0.047</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>house_beds_3</th>               <td>   -0.0028</td> <td>    0.053</td> <td>   -0.053</td> <td> 0.958</td> <td>   -0.107</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>house_kitchen_1</th>            <td>   -0.0111</td> <td>    0.036</td> <td>   -0.305</td> <td> 0.761</td> <td>   -0.082</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_1</th>   <td>    0.0315</td> <td>    0.156</td> <td>    0.202</td> <td> 0.840</td> <td>   -0.274</td> <td>    0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_2</th>   <td>    0.0184</td> <td>    0.155</td> <td>    0.118</td> <td> 0.906</td> <td>   -0.285</td> <td>    0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_3</th>   <td>    0.0160</td> <td>    0.156</td> <td>    0.103</td> <td> 0.918</td> <td>   -0.289</td> <td>    0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_4</th>   <td>    0.0159</td> <td>    0.158</td> <td>    0.101</td> <td> 0.920</td> <td>   -0.294</td> <td>    0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_5</th>   <td>   -0.0016</td> <td>    0.165</td> <td>   -0.010</td> <td> 0.992</td> <td>   -0.325</td> <td>    0.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_6</th>   <td>   -0.0432</td> <td>    0.215</td> <td>   -0.201</td> <td> 0.841</td> <td>   -0.465</td> <td>    0.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>got_pr_transf_1</th>            <td>   -0.0040</td> <td>    0.019</td> <td>   -0.210</td> <td> 0.834</td> <td>   -0.041</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>got_pu_transf_1</th>            <td>    0.0215</td> <td>    0.016</td> <td>    1.326</td> <td> 0.185</td> <td>   -0.010</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>part_in_training_1.0</th>       <td>    0.0117</td> <td>    0.020</td> <td>    0.574</td> <td> 0.566</td> <td>   -0.028</td> <td>    0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>offdays_ill_1.0</th>            <td>   -0.0501</td> <td>    0.040</td> <td>   -1.249</td> <td> 0.212</td> <td>   -0.129</td> <td>    0.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>healthinsurance_1.0</th>        <td>    0.0277</td> <td>    0.022</td> <td>    1.235</td> <td> 0.217</td> <td>   -0.016</td> <td>    0.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speak_other_languaje_1.0</th>   <td>   -0.0309</td> <td>    0.036</td> <td>   -0.860</td> <td> 0.390</td> <td>   -0.101</td> <td>    0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lives_w_mother_1.0</th>         <td>    0.0204</td> <td>    0.022</td> <td>    0.939</td> <td> 0.348</td> <td>   -0.022</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lives_w_father_1.0</th>         <td>   -0.0131</td> <td>    0.024</td> <td>   -0.554</td> <td> 0.580</td> <td>   -0.059</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>read_overall_1.0</th>           <td>    0.0262</td> <td>    0.028</td> <td>    0.945</td> <td> 0.345</td> <td>   -0.028</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>read_overall_2.0</th>           <td>    0.0259</td> <td>    0.030</td> <td>    0.873</td> <td> 0.383</td> <td>   -0.032</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>read_overall_3.0</th>           <td>    0.0299</td> <td>    0.030</td> <td>    1.002</td> <td> 0.317</td> <td>   -0.029</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>write_overall_1.0</th>          <td>   -0.0203</td> <td>    0.021</td> <td>   -0.948</td> <td> 0.343</td> <td>   -0.062</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>write_overall_2.0</th>          <td>   -0.0130</td> <td>    0.029</td> <td>   -0.456</td> <td> 0.649</td> <td>   -0.069</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>write_overall_3.0</th>          <td>   -0.0243</td> <td>    0.034</td> <td>   -0.716</td> <td> 0.474</td> <td>   -0.091</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numeracy_overall_1.0</th>       <td>   -0.0187</td> <td>    0.032</td> <td>   -0.588</td> <td> 0.557</td> <td>   -0.081</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numeracy_overall_2.0</th>       <td>   -0.0348</td> <td>    0.034</td> <td>   -1.031</td> <td> 0.302</td> <td>   -0.101</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numeracy_overall_3.0</th>       <td>    0.0040</td> <td>    0.037</td> <td>    0.109</td> <td> 0.913</td> <td>   -0.068</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>supervise_1.0</th>              <td>    0.0197</td> <td>    0.019</td> <td>    1.050</td> <td> 0.294</td> <td>   -0.017</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>computer_use_overall_1.0</th>   <td>   -0.0057</td> <td>    0.028</td> <td>   -0.206</td> <td> 0.837</td> <td>   -0.060</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>computer_use_overall_2.0</th>   <td>   -0.0004</td> <td>    0.028</td> <td>   -0.014</td> <td> 0.989</td> <td>   -0.055</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>computer_use_overall_3.0</th>   <td>   -0.0112</td> <td>    0.022</td> <td>   -0.497</td> <td> 0.619</td> <td>   -0.055</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>think_learn_work_1.0</th>       <td>    0.0102</td> <td>    0.027</td> <td>    0.381</td> <td> 0.703</td> <td>   -0.042</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>think_learn_work_2.0</th>       <td>    0.0021</td> <td>    0.025</td> <td>    0.083</td> <td> 0.934</td> <td>   -0.047</td> <td>    0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>think_learn_work_3.0</th>       <td>    0.0080</td> <td>    0.027</td> <td>    0.293</td> <td> 0.769</td> <td>   -0.046</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>autonomy_at_work_1.0</th>       <td>   -0.0305</td> <td>    0.033</td> <td>   -0.915</td> <td> 0.360</td> <td>   -0.096</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>autonomy_at_work_2.0</th>       <td>   -0.0183</td> <td>    0.032</td> <td>   -0.564</td> <td> 0.573</td> <td>   -0.082</td> <td>    0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>autonomy_at_work_3.0</th>       <td>    0.0179</td> <td>    0.033</td> <td>    0.548</td> <td> 0.584</td> <td>   -0.046</td> <td>    0.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>repetitiveness_at_work_1.0</th> <td>    0.0338</td> <td>    0.027</td> <td>    1.273</td> <td> 0.203</td> <td>   -0.018</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>repetitiveness_at_work_2.0</th> <td>   -0.0045</td> <td>    0.032</td> <td>   -0.141</td> <td> 0.888</td> <td>   -0.067</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>repetitiveness_at_work_3.0</th> <td>    0.0130</td> <td>    0.039</td> <td>    0.332</td> <td> 0.740</td> <td>   -0.063</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>physical_demand_work_1.0</th>   <td>   -0.0361</td> <td>    0.029</td> <td>   -1.231</td> <td> 0.218</td> <td>   -0.094</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>physical_demand_work_2.0</th>   <td>   -0.0253</td> <td>    0.030</td> <td>   -0.850</td> <td> 0.395</td> <td>   -0.084</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>physical_demand_work_3.0</th>   <td>   -0.0428</td> <td>    0.029</td> <td>   -1.468</td> <td> 0.142</td> <td>   -0.100</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_children_1.0</th>           <td>   -0.0404</td> <td>    0.019</td> <td>   -2.104</td> <td> 0.035</td> <td>   -0.078</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hh_size_2</th>                  <td>   -0.0201</td> <td>    0.018</td> <td>   -1.125</td> <td> 0.261</td> <td>   -0.055</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender_1</th>                   <td>    0.0033</td> <td>    0.017</td> <td>    0.196</td> <td> 0.845</td> <td>   -0.029</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_spouse_1</th>               <td>    0.0499</td> <td>    0.019</td> <td>    2.651</td> <td> 0.008</td> <td>    0.013</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chronic_disease_1.0</th>        <td>   -0.0322</td> <td>    0.022</td> <td>   -1.469</td> <td> 0.142</td> <td>   -0.075</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shocks_bef_15_1.0</th>          <td>   -0.0151</td> <td>    0.015</td> <td>   -0.998</td> <td> 0.318</td> <td>   -0.045</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mother_tongue_1.0</th>          <td>   -0.0298</td> <td>    0.164</td> <td>   -0.182</td> <td> 0.856</td> <td>   -0.351</td> <td>    0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>labor_market_status_1.0</th>    <td>    0.0353</td> <td>    0.030</td> <td>    1.161</td> <td> 0.246</td> <td>   -0.024</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>job_stable_1.0</th>             <td>    0.0056</td> <td>    0.019</td> <td>    0.301</td> <td> 0.763</td> <td>   -0.031</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_1</th>      <td>   -0.0108</td> <td>    0.031</td> <td>   -0.352</td> <td> 0.725</td> <td>   -0.071</td> <td>    0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_2</th>      <td>   -0.0072</td> <td>    0.043</td> <td>   -0.168</td> <td> 0.866</td> <td>   -0.092</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_3</th>      <td>    0.0106</td> <td>    0.043</td> <td>    0.249</td> <td> 0.803</td> <td>   -0.073</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_5</th>      <td>    0.0321</td> <td>    0.058</td> <td>    0.558</td> <td> 0.577</td> <td>   -0.081</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_6</th>      <td>    0.0211</td> <td>    0.068</td> <td>    0.311</td> <td> 0.756</td> <td>   -0.112</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_7</th>      <td>   -0.0579</td> <td>    0.101</td> <td>   -0.574</td> <td> 0.566</td> <td>   -0.256</td> <td>    0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_8</th>      <td>    0.0156</td> <td>    0.162</td> <td>    0.096</td> <td> 0.923</td> <td>   -0.302</td> <td>    0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dropout_1.0</th>                <td>   -0.0059</td> <td>    0.010</td> <td>   -0.567</td> <td> 0.571</td> <td>   -0.026</td> <td>    0.015</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:      life_satisfaction   No. Observations:                 2617\n",
       "Model:                            GLM   Df Residuals:                     2537\n",
       "Model Family:                 Poisson   Df Model:                           79\n",
       "Link Function:                    log   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -5665.4\n",
       "Date:                Thu, 19 Mar 2020   Deviance:                       1172.2\n",
       "Time:                        15:17:52   Pearson chi2:                 1.06e+03\n",
       "No. Iterations:                     4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "const                          2.0954      0.251      8.361      0.000       1.604       2.587\n",
       "ses_15                         0.0143      0.004      3.274      0.001       0.006       0.023\n",
       "extraversion_av               -0.0004      0.011     -0.034      0.973      -0.022       0.021\n",
       "conscientiousness_avg         -0.0212      0.015     -1.406      0.160      -0.051       0.008\n",
       "openness_av                    0.0110      0.015      0.728      0.466      -0.019       0.041\n",
       "stability_av                   0.0424      0.010      4.041      0.000       0.022       0.063\n",
       "agreeableness_av               0.0105      0.013      0.794      0.427      -0.015       0.036\n",
       "grit_av                        0.0105      0.013      0.820      0.412      -0.015       0.035\n",
       "decision_av                   -0.0250      0.013     -1.907      0.056      -0.051       0.001\n",
       "hostile_av                    -0.0331      0.012     -2.669      0.008      -0.057      -0.009\n",
       "risk                          -0.0043      0.007     -0.621      0.535      -0.018       0.009\n",
       "age                           -0.0005      0.001     -0.634      0.526      -0.002       0.001\n",
       "BMI                           -0.0008      0.002     -0.398      0.691      -0.004       0.003\n",
       "hour_earnings              -7.581e-08   7.05e-07     -0.107      0.914   -1.46e-06    1.31e-06\n",
       "total_hr_worked_week        -4.85e-05      0.000     -0.120      0.904      -0.001       0.001\n",
       "highest_ISCED_PIAAC            0.0131      0.022      0.603      0.547      -0.029       0.056\n",
       "years_educ                    -0.0074      0.008     -0.930      0.352      -0.023       0.008\n",
       "wealth_index                   0.0347      0.012      2.991      0.003       0.012       0.057\n",
       "overqualified                 -0.0145      0.008     -1.879      0.060      -0.030       0.001\n",
       "dropout                       -0.0059      0.010     -0.567      0.571      -0.026       0.015\n",
       "in_school_1.0                  0.0255      0.026      0.983      0.326      -0.025       0.076\n",
       "owns_house_2                  -0.0071      0.018     -0.404      0.686      -0.042       0.027\n",
       "owns_house_3                  -0.0401      0.029     -1.381      0.167      -0.097       0.017\n",
       "house_beds_2                  -0.0126      0.018     -0.718      0.473      -0.047       0.022\n",
       "house_beds_3                  -0.0028      0.053     -0.053      0.958      -0.107       0.102\n",
       "house_kitchen_1               -0.0111      0.036     -0.305      0.761      -0.082       0.060\n",
       "reported_social_status_1       0.0315      0.156      0.202      0.840      -0.274       0.337\n",
       "reported_social_status_2       0.0184      0.155      0.118      0.906      -0.285       0.322\n",
       "reported_social_status_3       0.0160      0.156      0.103      0.918      -0.289       0.321\n",
       "reported_social_status_4       0.0159      0.158      0.101      0.920      -0.294       0.326\n",
       "reported_social_status_5      -0.0016      0.165     -0.010      0.992      -0.325       0.322\n",
       "reported_social_status_6      -0.0432      0.215     -0.201      0.841      -0.465       0.378\n",
       "got_pr_transf_1               -0.0040      0.019     -0.210      0.834      -0.041       0.033\n",
       "got_pu_transf_1                0.0215      0.016      1.326      0.185      -0.010       0.053\n",
       "part_in_training_1.0           0.0117      0.020      0.574      0.566      -0.028       0.052\n",
       "offdays_ill_1.0               -0.0501      0.040     -1.249      0.212      -0.129       0.029\n",
       "healthinsurance_1.0            0.0277      0.022      1.235      0.217      -0.016       0.072\n",
       "speak_other_languaje_1.0      -0.0309      0.036     -0.860      0.390      -0.101       0.040\n",
       "lives_w_mother_1.0             0.0204      0.022      0.939      0.348      -0.022       0.063\n",
       "lives_w_father_1.0            -0.0131      0.024     -0.554      0.580      -0.059       0.033\n",
       "read_overall_1.0               0.0262      0.028      0.945      0.345      -0.028       0.081\n",
       "read_overall_2.0               0.0259      0.030      0.873      0.383      -0.032       0.084\n",
       "read_overall_3.0               0.0299      0.030      1.002      0.317      -0.029       0.088\n",
       "write_overall_1.0             -0.0203      0.021     -0.948      0.343      -0.062       0.022\n",
       "write_overall_2.0             -0.0130      0.029     -0.456      0.649      -0.069       0.043\n",
       "write_overall_3.0             -0.0243      0.034     -0.716      0.474      -0.091       0.042\n",
       "numeracy_overall_1.0          -0.0187      0.032     -0.588      0.557      -0.081       0.044\n",
       "numeracy_overall_2.0          -0.0348      0.034     -1.031      0.302      -0.101       0.031\n",
       "numeracy_overall_3.0           0.0040      0.037      0.109      0.913      -0.068       0.076\n",
       "supervise_1.0                  0.0197      0.019      1.050      0.294      -0.017       0.057\n",
       "computer_use_overall_1.0      -0.0057      0.028     -0.206      0.837      -0.060       0.048\n",
       "computer_use_overall_2.0      -0.0004      0.028     -0.014      0.989      -0.055       0.054\n",
       "computer_use_overall_3.0      -0.0112      0.022     -0.497      0.619      -0.055       0.033\n",
       "think_learn_work_1.0           0.0102      0.027      0.381      0.703      -0.042       0.063\n",
       "think_learn_work_2.0           0.0021      0.025      0.083      0.934      -0.047       0.051\n",
       "think_learn_work_3.0           0.0080      0.027      0.293      0.769      -0.046       0.062\n",
       "autonomy_at_work_1.0          -0.0305      0.033     -0.915      0.360      -0.096       0.035\n",
       "autonomy_at_work_2.0          -0.0183      0.032     -0.564      0.573      -0.082       0.045\n",
       "autonomy_at_work_3.0           0.0179      0.033      0.548      0.584      -0.046       0.082\n",
       "repetitiveness_at_work_1.0     0.0338      0.027      1.273      0.203      -0.018       0.086\n",
       "repetitiveness_at_work_2.0    -0.0045      0.032     -0.141      0.888      -0.067       0.058\n",
       "repetitiveness_at_work_3.0     0.0130      0.039      0.332      0.740      -0.063       0.089\n",
       "physical_demand_work_1.0      -0.0361      0.029     -1.231      0.218      -0.094       0.021\n",
       "physical_demand_work_2.0      -0.0253      0.030     -0.850      0.395      -0.084       0.033\n",
       "physical_demand_work_3.0      -0.0428      0.029     -1.468      0.142      -0.100       0.014\n",
       "has_children_1.0              -0.0404      0.019     -2.104      0.035      -0.078      -0.003\n",
       "hh_size_2                     -0.0201      0.018     -1.125      0.261      -0.055       0.015\n",
       "gender_1                       0.0033      0.017      0.196      0.845      -0.029       0.036\n",
       "has_spouse_1                   0.0499      0.019      2.651      0.008       0.013       0.087\n",
       "chronic_disease_1.0           -0.0322      0.022     -1.469      0.142      -0.075       0.011\n",
       "shocks_bef_15_1.0             -0.0151      0.015     -0.998      0.318      -0.045       0.015\n",
       "mother_tongue_1.0             -0.0298      0.164     -0.182      0.856      -0.351       0.291\n",
       "labor_market_status_1.0        0.0353      0.030      1.161      0.246      -0.024       0.095\n",
       "job_stable_1.0                 0.0056      0.019      0.301      0.763      -0.031       0.042\n",
       "highest_ISCED_PIAAC_1         -0.0108      0.031     -0.352      0.725      -0.071       0.049\n",
       "highest_ISCED_PIAAC_2         -0.0072      0.043     -0.168      0.866      -0.092       0.077\n",
       "highest_ISCED_PIAAC_3          0.0106      0.043      0.249      0.803      -0.073       0.094\n",
       "highest_ISCED_PIAAC_5          0.0321      0.058      0.558      0.577      -0.081       0.145\n",
       "highest_ISCED_PIAAC_6          0.0211      0.068      0.311      0.756      -0.112       0.154\n",
       "highest_ISCED_PIAAC_7         -0.0579      0.101     -0.574      0.566      -0.256       0.140\n",
       "highest_ISCED_PIAAC_8          0.0156      0.162      0.096      0.923      -0.302       0.333\n",
       "dropout_1.0                   -0.0059      0.010     -0.567      0.571      -0.026       0.015\n",
       "==============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting coefficients \n",
    "poi_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = {'ses_15': 0.0143, 'extraversion_av': -0.0004, 'conscientiousness_avg': -0.0212, \n",
    "                'openness_av': 0.0110, 'stability_av': 0.0424, 'agreeableness_av': 0.0105, \n",
    "                'grit_av': 0.0105, 'decision_av':-0.0250, 'hostile_av':-0.0331, 'risk': -0.0043, \n",
    "                'age': -0.0005, 'BMI':-0.0008, 'hour_earnings':-7.581e-08, 'total_hr_worked_week': -4.85e-05, \n",
    "                'highest_ISCED_PIAAC': 0.0131, 'years_educ': -0.0074, 'wealth_index': 0.0347, \n",
    "                'overqualified': -0.0145, 'dropout': -0.0059, 'in_school_1.0': 0.0255, 'owns_house_2': -0.0071, \n",
    "                'owns_house_3': -0.0401, 'house_beds_2': -0.0126, 'house_beds_3': -0.0028, \n",
    "                'house_kitchen_1': -0.0111, 'reported_social_status_1': 0.0315, \n",
    "                'reported_social_status_2': 0.0184, 'reported_social_status_3': 0.0160, \n",
    "                'reported_social_status_4': 0.0159, 'reported_social_status_5': -0.0016, \n",
    "                'reported_social_status_6': -0.0432, 'got_pr_transf_1':-0.0040, 'got_pu_transf_1': 0.0215, \n",
    "                'part_in_training_1.0': 0.0117, 'offdays_ill_1.0': -0.0501, 'healthinsurance_1.0': 0.0277, \n",
    "                'speak_other_languaje_1.0': -0.0309, 'lives_w_mother_1.0': 0.0204, 'lives_w_father_1.0': -0.0131, \n",
    "                'read_overall_1.0': 0.0262, 'read_overall_2.0': 0.0259, 'read_overall_3.0': 0.0299, \n",
    "                'write_overall_1.0': -0.0203, 'write_overall_2.0': -0.0130, 'write_overall_3.0': -0.0243, \n",
    "                'numeracy_overall_1.0': -0.0187, 'numeracy_overall_2.0': -0.0348, \n",
    "                'numeracy_overall_3.0': 0.0040, 'supervise_1.0': 0.0197, 'computer_use_overall_1.0': -0.0057, \n",
    "                'computer_use_overall_2.0': -0.0004, 'computer_use_overall_3.0': -0.0112, \n",
    "                'think_learn_work_1.0': 0.0102, 'think_learn_work_2.0': 0.0021, 'think_learn_work_3.0': 0.0080, \n",
    "                'autonomy_at_work_1.0': -0.0305, 'autonomy_at_work_2.0': -0.0183, 'autonomy_at_work_3.0': 0.0179,\n",
    "                'repetitiveness_at_work_1.0': 0.0338, 'repetitiveness_at_work_2.0': -0.0045, \n",
    "                'repetitiveness_at_work_3.0': 0.0130, 'physical_demand_work_1.0': -0.0361, 'physical_demand_work_2.0': -0.0253, \n",
    "                'physical_demand_work_3.0': -0.0428, 'has_children_1.0': -0.0404, 'hh_size_2': -0.0201, 'gender_1': 0.0033, \n",
    "                'has_spouse_1': 0.0499, 'chronic_disease_1.0': -0.0322, 'shocks_bef_15_1.0': -0.0151, \n",
    "                'mother_tongue_1.0': -0.0298, 'labor_market_status_1.0': 0.0353, 'job_stable_1.0': 0.0056, \n",
    "                'highest_ISCED_PIAAC_1': -0.0108, 'highest_ISCED_PIAAC_2': -0.0072, 'highest_ISCED_PIAAC_3': 0.0106,\n",
    "                'highest_ISCED_PIAAC_5': 0.0321, 'highest_ISCED_PIAAC_6': 0.0211, 'highest_ISCED_PIAAC_7': -0.0579, \n",
    "                'highest_ISCED_PIAAC_8': 0.0156, 'dropout_1.0': -0.0059}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "stability_av 0.0424\n",
      "1.043311719984947\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "wealth_index 0.0347\n",
      "1.0353090694852103\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "reported_social_status_1 0.0315\n",
      "1.0320013755956459\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "read_overall_3.0 0.0299\n",
      "1.0303514936522224\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "repetitiveness_at_work_1.0 0.0338\n",
      "1.034377710497085\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "has_spouse_1 0.0499\n",
      "1.0511659745225668\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "labor_market_status_1.0 0.0353\n",
      "1.0359304413198107\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "highest_ISCED_PIAAC_5 0.0321\n",
      "1.0326207622184085\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n"
     ]
    }
   ],
   "source": [
    "# The list below shows that changes in 8 variables correspond to increases in happiness of between a 3% and 5%. Other \n",
    "# variables have a smaller impact. It is possible that the poisson regression is not the best model to identify \n",
    "\n",
    "for item, value in coefficients.items(): \n",
    "    coeff = np.exp(value)\n",
    "    if coeff >= 1.03: \n",
    "        print(item,value)\n",
    "        print(coeff)\n",
    "    else: \n",
    "        print('No impact in model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting logistic regression on binarized Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column to binarize \n",
    "colstep['binary_life_satisfaction'] = colstep['life_satisfaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarizing so that 0 = 5 or less, and 1 = 6 or more \n",
    "unhappy = [1,2,3,4,5]\n",
    "happy = [6,7,8,9,10]\n",
    "\n",
    "for number in unhappy: \n",
    "    colstep['binary_life_satisfaction'].replace(number,0, inplace=True)\n",
    "    \n",
    "for number in happy:\n",
    "    colstep['binary_life_satisfaction'].replace(number,1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    2276\n",
       "0.0     341\n",
       "Name: binary_life_satisfaction, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming new values of y\n",
    "colstep['binary_life_satisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting y for as a binary variable \n",
    "yb = colstep['binary_life_satisfaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vvroseth/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Calling Family(..) with a link class as argument is deprecated.\n",
      "Use an instance of a link class instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Instantiating and fiting the model using stats models  \n",
    "logreg = sm.GLM(yb, X, \n",
    "                family=sm.families.Binomial(link = sm.families.links.logit)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>binary_life_satisfaction</td> <th>  No. Observations:  </th>  <td>  2617</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                      <td>GLM</td>           <th>  Df Residuals:      </th>  <td>  2537</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>            <td>Binomial</td>         <th>  Df Model:          </th>  <td>    79</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>             <td>logit</td>          <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                    <td>IRLS</td>           <th>  Log-Likelihood:    </th> <td> -848.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>                <td>Fri, 20 Mar 2020</td>     <th>  Deviance:          </th> <td>  1697.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                    <td>15:20:25</td>         <th>  Pearson chi2:      </th> <td>2.51e+03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>             <td>19</td>            <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>nonrobust</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                      <td>    1.2562</td> <td>    1.870</td> <td>    0.672</td> <td> 0.502</td> <td>   -2.408</td> <td>    4.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ses_15</th>                     <td>    0.1929</td> <td>    0.041</td> <td>    4.758</td> <td> 0.000</td> <td>    0.113</td> <td>    0.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>extraversion_av</th>            <td>   -0.1406</td> <td>    0.096</td> <td>   -1.465</td> <td> 0.143</td> <td>   -0.329</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>conscientiousness_avg</th>      <td>   -0.1809</td> <td>    0.139</td> <td>   -1.297</td> <td> 0.195</td> <td>   -0.454</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>openness_av</th>                <td>    0.1991</td> <td>    0.129</td> <td>    1.541</td> <td> 0.123</td> <td>   -0.054</td> <td>    0.452</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>stability_av</th>               <td>    0.4002</td> <td>    0.094</td> <td>    4.240</td> <td> 0.000</td> <td>    0.215</td> <td>    0.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>agreeableness_av</th>           <td>    0.0124</td> <td>    0.116</td> <td>    0.107</td> <td> 0.915</td> <td>   -0.215</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grit_av</th>                    <td>    0.0207</td> <td>    0.113</td> <td>    0.184</td> <td> 0.854</td> <td>   -0.200</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>decision_av</th>                <td>   -0.2189</td> <td>    0.115</td> <td>   -1.900</td> <td> 0.057</td> <td>   -0.445</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hostile_av</th>                 <td>   -0.4082</td> <td>    0.104</td> <td>   -3.936</td> <td> 0.000</td> <td>   -0.612</td> <td>   -0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>risk</th>                       <td>   -0.0219</td> <td>    0.064</td> <td>   -0.343</td> <td> 0.731</td> <td>   -0.147</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                        <td>   -0.0027</td> <td>    0.007</td> <td>   -0.398</td> <td> 0.690</td> <td>   -0.016</td> <td>    0.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BMI</th>                        <td>   -0.0122</td> <td>    0.016</td> <td>   -0.764</td> <td> 0.445</td> <td>   -0.043</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hour_earnings</th>              <td> 8.801e-06</td> <td>    1e-05</td> <td>    0.877</td> <td> 0.381</td> <td>-1.09e-05</td> <td> 2.85e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total_hr_worked_week</th>       <td>   -0.0018</td> <td>    0.004</td> <td>   -0.503</td> <td> 0.615</td> <td>   -0.009</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC</th>        <td>    0.8402</td> <td>  530.321</td> <td>    0.002</td> <td> 0.999</td> <td>-1038.570</td> <td> 1040.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>years_educ</th>                 <td>   -0.0605</td> <td>    0.074</td> <td>   -0.822</td> <td> 0.411</td> <td>   -0.205</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wealth_index</th>               <td>    0.4449</td> <td>    0.103</td> <td>    4.323</td> <td> 0.000</td> <td>    0.243</td> <td>    0.647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>overqualified</th>              <td>   -0.1388</td> <td>    0.065</td> <td>   -2.147</td> <td> 0.032</td> <td>   -0.265</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dropout</th>                    <td>   -0.0872</td> <td>    0.090</td> <td>   -0.966</td> <td> 0.334</td> <td>   -0.264</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>in_school_1.0</th>              <td>    0.3564</td> <td>    0.299</td> <td>    1.190</td> <td> 0.234</td> <td>   -0.230</td> <td>    0.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>owns_house_2</th>               <td>    0.0983</td> <td>    0.161</td> <td>    0.609</td> <td> 0.542</td> <td>   -0.218</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>owns_house_3</th>               <td>   -0.1115</td> <td>    0.245</td> <td>   -0.455</td> <td> 0.649</td> <td>   -0.592</td> <td>    0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>house_beds_2</th>               <td>   -0.2382</td> <td>    0.167</td> <td>   -1.428</td> <td> 0.153</td> <td>   -0.565</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>house_beds_3</th>               <td>   -0.2529</td> <td>    0.530</td> <td>   -0.477</td> <td> 0.633</td> <td>   -1.291</td> <td>    0.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>house_kitchen_1</th>            <td>   -0.3204</td> <td>    0.286</td> <td>   -1.119</td> <td> 0.263</td> <td>   -0.882</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_1</th>   <td>    0.6997</td> <td>    1.027</td> <td>    0.681</td> <td> 0.496</td> <td>   -1.314</td> <td>    2.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_2</th>   <td>    0.4566</td> <td>    1.017</td> <td>    0.449</td> <td> 0.654</td> <td>   -1.537</td> <td>    2.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_3</th>   <td>    0.6023</td> <td>    1.026</td> <td>    0.587</td> <td> 0.557</td> <td>   -1.408</td> <td>    2.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_4</th>   <td>    0.1483</td> <td>    1.078</td> <td>    0.138</td> <td> 0.891</td> <td>   -1.964</td> <td>    2.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_5</th>   <td>    0.5546</td> <td>    1.225</td> <td>    0.453</td> <td> 0.651</td> <td>   -1.847</td> <td>    2.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reported_social_status_6</th>   <td>   -0.9574</td> <td>    1.574</td> <td>   -0.608</td> <td> 0.543</td> <td>   -4.042</td> <td>    2.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>got_pr_transf_1</th>            <td>    0.0024</td> <td>    0.176</td> <td>    0.014</td> <td> 0.989</td> <td>   -0.342</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>got_pu_transf_1</th>            <td>    0.1019</td> <td>    0.145</td> <td>    0.703</td> <td> 0.482</td> <td>   -0.182</td> <td>    0.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>part_in_training_1.0</th>       <td>    0.0917</td> <td>    0.209</td> <td>    0.439</td> <td> 0.661</td> <td>   -0.318</td> <td>    0.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>offdays_ill_1.0</th>            <td>    0.0517</td> <td>    0.284</td> <td>    0.182</td> <td> 0.855</td> <td>   -0.504</td> <td>    0.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>healthinsurance_1.0</th>        <td>    0.4850</td> <td>    0.175</td> <td>    2.771</td> <td> 0.006</td> <td>    0.142</td> <td>    0.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>speak_other_languaje_1.0</th>   <td>   -0.1868</td> <td>    0.447</td> <td>   -0.418</td> <td> 0.676</td> <td>   -1.063</td> <td>    0.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lives_w_mother_1.0</th>         <td>    0.2421</td> <td>    0.206</td> <td>    1.176</td> <td> 0.240</td> <td>   -0.161</td> <td>    0.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lives_w_father_1.0</th>         <td>    0.0442</td> <td>    0.248</td> <td>    0.178</td> <td> 0.859</td> <td>   -0.442</td> <td>    0.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>read_overall_1.0</th>           <td>    0.1261</td> <td>    0.212</td> <td>    0.596</td> <td> 0.551</td> <td>   -0.289</td> <td>    0.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>read_overall_2.0</th>           <td>    0.3984</td> <td>    0.239</td> <td>    1.666</td> <td> 0.096</td> <td>   -0.070</td> <td>    0.867</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>read_overall_3.0</th>           <td>    0.2478</td> <td>    0.243</td> <td>    1.020</td> <td> 0.308</td> <td>   -0.228</td> <td>    0.724</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>write_overall_1.0</th>          <td>    0.0905</td> <td>    0.175</td> <td>    0.517</td> <td> 0.605</td> <td>   -0.252</td> <td>    0.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>write_overall_2.0</th>          <td>    0.3762</td> <td>    0.277</td> <td>    1.360</td> <td> 0.174</td> <td>   -0.166</td> <td>    0.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>write_overall_3.0</th>          <td>    0.4217</td> <td>    0.356</td> <td>    1.184</td> <td> 0.237</td> <td>   -0.277</td> <td>    1.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numeracy_overall_1.0</th>       <td>    0.1176</td> <td>    0.249</td> <td>    0.472</td> <td> 0.637</td> <td>   -0.371</td> <td>    0.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numeracy_overall_2.0</th>       <td>   -0.0822</td> <td>    0.274</td> <td>   -0.300</td> <td> 0.764</td> <td>   -0.620</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>numeracy_overall_3.0</th>       <td>    0.1395</td> <td>    0.338</td> <td>    0.413</td> <td> 0.680</td> <td>   -0.523</td> <td>    0.802</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>supervise_1.0</th>              <td>    0.0888</td> <td>    0.179</td> <td>    0.495</td> <td> 0.621</td> <td>   -0.263</td> <td>    0.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>computer_use_overall_1.0</th>   <td>    0.2742</td> <td>    0.267</td> <td>    1.027</td> <td> 0.305</td> <td>   -0.249</td> <td>    0.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>computer_use_overall_2.0</th>   <td>   -0.2644</td> <td>    0.246</td> <td>   -1.075</td> <td> 0.283</td> <td>   -0.746</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>computer_use_overall_3.0</th>   <td>   -0.1024</td> <td>    0.215</td> <td>   -0.476</td> <td> 0.634</td> <td>   -0.524</td> <td>    0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>think_learn_work_1.0</th>       <td>    0.1372</td> <td>    0.224</td> <td>    0.611</td> <td> 0.541</td> <td>   -0.303</td> <td>    0.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>think_learn_work_2.0</th>       <td>    0.0733</td> <td>    0.210</td> <td>    0.350</td> <td> 0.727</td> <td>   -0.338</td> <td>    0.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>think_learn_work_3.0</th>       <td>    0.1505</td> <td>    0.242</td> <td>    0.621</td> <td> 0.535</td> <td>   -0.325</td> <td>    0.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>autonomy_at_work_1.0</th>       <td>   -0.4567</td> <td>    0.304</td> <td>   -1.502</td> <td> 0.133</td> <td>   -1.053</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>autonomy_at_work_2.0</th>       <td>   -0.3696</td> <td>    0.296</td> <td>   -1.247</td> <td> 0.212</td> <td>   -0.950</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>autonomy_at_work_3.0</th>       <td>   -0.1993</td> <td>    0.298</td> <td>   -0.669</td> <td> 0.504</td> <td>   -0.783</td> <td>    0.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>repetitiveness_at_work_1.0</th> <td>    0.1006</td> <td>    0.251</td> <td>    0.401</td> <td> 0.689</td> <td>   -0.391</td> <td>    0.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>repetitiveness_at_work_2.0</th> <td>    0.0672</td> <td>    0.321</td> <td>    0.209</td> <td> 0.834</td> <td>   -0.562</td> <td>    0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>repetitiveness_at_work_3.0</th> <td>    0.2068</td> <td>    0.397</td> <td>    0.521</td> <td> 0.602</td> <td>   -0.571</td> <td>    0.985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>physical_demand_work_1.0</th>   <td>   -0.3446</td> <td>    0.303</td> <td>   -1.137</td> <td> 0.255</td> <td>   -0.939</td> <td>    0.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>physical_demand_work_2.0</th>   <td>   -0.2605</td> <td>    0.303</td> <td>   -0.859</td> <td> 0.390</td> <td>   -0.854</td> <td>    0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>physical_demand_work_3.0</th>   <td>   -0.3868</td> <td>    0.293</td> <td>   -1.320</td> <td> 0.187</td> <td>   -0.961</td> <td>    0.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_children_1.0</th>           <td>   -0.3143</td> <td>    0.168</td> <td>   -1.876</td> <td> 0.061</td> <td>   -0.643</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hh_size_2</th>                  <td>   -0.1484</td> <td>    0.158</td> <td>   -0.938</td> <td> 0.348</td> <td>   -0.458</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender_1</th>                   <td>   -0.0578</td> <td>    0.159</td> <td>   -0.364</td> <td> 0.716</td> <td>   -0.369</td> <td>    0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>has_spouse_1</th>               <td>    0.4047</td> <td>    0.156</td> <td>    2.586</td> <td> 0.010</td> <td>    0.098</td> <td>    0.711</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chronic_disease_1.0</th>        <td>   -0.3579</td> <td>    0.178</td> <td>   -2.009</td> <td> 0.044</td> <td>   -0.707</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>shocks_bef_15_1.0</th>          <td>   -0.1890</td> <td>    0.138</td> <td>   -1.373</td> <td> 0.170</td> <td>   -0.459</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mother_tongue_1.0</th>          <td>    0.4931</td> <td>    1.286</td> <td>    0.384</td> <td> 0.701</td> <td>   -2.027</td> <td>    3.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>labor_market_status_1.0</th>    <td>    0.4203</td> <td>    0.264</td> <td>    1.593</td> <td> 0.111</td> <td>   -0.097</td> <td>    0.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>job_stable_1.0</th>             <td>    0.3112</td> <td>    0.169</td> <td>    1.845</td> <td> 0.065</td> <td>   -0.019</td> <td>    0.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_1</th>      <td>   -0.5252</td> <td>  530.321</td> <td>   -0.001</td> <td> 0.999</td> <td>-1039.936</td> <td> 1038.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_2</th>      <td>   -1.4265</td> <td> 1060.642</td> <td>   -0.001</td> <td> 0.999</td> <td>-2080.247</td> <td> 2077.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_3</th>      <td>   -1.9327</td> <td> 1590.963</td> <td>   -0.001</td> <td> 0.999</td> <td>-3120.164</td> <td> 3116.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_5</th>      <td>   -2.7007</td> <td> 2651.606</td> <td>   -0.001</td> <td> 0.999</td> <td>-5199.753</td> <td> 5194.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_6</th>      <td>   -3.9744</td> <td> 3181.927</td> <td>   -0.001</td> <td> 0.999</td> <td>-6240.437</td> <td> 6232.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_7</th>      <td>   -5.4589</td> <td> 3712.248</td> <td>   -0.001</td> <td> 0.999</td> <td>-7281.332</td> <td> 7270.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>highest_ISCED_PIAAC_8</th>      <td>   10.6973</td> <td> 8286.268</td> <td>    0.001</td> <td> 0.999</td> <td>-1.62e+04</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dropout_1.0</th>                <td>   -0.0872</td> <td>    0.090</td> <td>   -0.966</td> <td> 0.334</td> <td>   -0.264</td> <td>    0.090</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                    Generalized Linear Model Regression Results                     \n",
       "====================================================================================\n",
       "Dep. Variable:     binary_life_satisfaction   No. Observations:                 2617\n",
       "Model:                                  GLM   Df Residuals:                     2537\n",
       "Model Family:                      Binomial   Df Model:                           79\n",
       "Link Function:                        logit   Scale:                          1.0000\n",
       "Method:                                IRLS   Log-Likelihood:                -848.57\n",
       "Date:                      Fri, 20 Mar 2020   Deviance:                       1697.1\n",
       "Time:                              15:20:25   Pearson chi2:                 2.51e+03\n",
       "No. Iterations:                          19                                         \n",
       "Covariance Type:                  nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "const                          1.2562      1.870      0.672      0.502      -2.408       4.920\n",
       "ses_15                         0.1929      0.041      4.758      0.000       0.113       0.272\n",
       "extraversion_av               -0.1406      0.096     -1.465      0.143      -0.329       0.048\n",
       "conscientiousness_avg         -0.1809      0.139     -1.297      0.195      -0.454       0.092\n",
       "openness_av                    0.1991      0.129      1.541      0.123      -0.054       0.452\n",
       "stability_av                   0.4002      0.094      4.240      0.000       0.215       0.585\n",
       "agreeableness_av               0.0124      0.116      0.107      0.915      -0.215       0.240\n",
       "grit_av                        0.0207      0.113      0.184      0.854      -0.200       0.242\n",
       "decision_av                   -0.2189      0.115     -1.900      0.057      -0.445       0.007\n",
       "hostile_av                    -0.4082      0.104     -3.936      0.000      -0.612      -0.205\n",
       "risk                          -0.0219      0.064     -0.343      0.731      -0.147       0.103\n",
       "age                           -0.0027      0.007     -0.398      0.690      -0.016       0.011\n",
       "BMI                           -0.0122      0.016     -0.764      0.445      -0.043       0.019\n",
       "hour_earnings               8.801e-06      1e-05      0.877      0.381   -1.09e-05    2.85e-05\n",
       "total_hr_worked_week          -0.0018      0.004     -0.503      0.615      -0.009       0.005\n",
       "highest_ISCED_PIAAC            0.8402    530.321      0.002      0.999   -1038.570    1040.251\n",
       "years_educ                    -0.0605      0.074     -0.822      0.411      -0.205       0.084\n",
       "wealth_index                   0.4449      0.103      4.323      0.000       0.243       0.647\n",
       "overqualified                 -0.1388      0.065     -2.147      0.032      -0.265      -0.012\n",
       "dropout                       -0.0872      0.090     -0.966      0.334      -0.264       0.090\n",
       "in_school_1.0                  0.3564      0.299      1.190      0.234      -0.230       0.943\n",
       "owns_house_2                   0.0983      0.161      0.609      0.542      -0.218       0.415\n",
       "owns_house_3                  -0.1115      0.245     -0.455      0.649      -0.592       0.369\n",
       "house_beds_2                  -0.2382      0.167     -1.428      0.153      -0.565       0.089\n",
       "house_beds_3                  -0.2529      0.530     -0.477      0.633      -1.291       0.785\n",
       "house_kitchen_1               -0.3204      0.286     -1.119      0.263      -0.882       0.241\n",
       "reported_social_status_1       0.6997      1.027      0.681      0.496      -1.314       2.713\n",
       "reported_social_status_2       0.4566      1.017      0.449      0.654      -1.537       2.450\n",
       "reported_social_status_3       0.6023      1.026      0.587      0.557      -1.408       2.612\n",
       "reported_social_status_4       0.1483      1.078      0.138      0.891      -1.964       2.261\n",
       "reported_social_status_5       0.5546      1.225      0.453      0.651      -1.847       2.956\n",
       "reported_social_status_6      -0.9574      1.574     -0.608      0.543      -4.042       2.128\n",
       "got_pr_transf_1                0.0024      0.176      0.014      0.989      -0.342       0.347\n",
       "got_pu_transf_1                0.1019      0.145      0.703      0.482      -0.182       0.386\n",
       "part_in_training_1.0           0.0917      0.209      0.439      0.661      -0.318       0.502\n",
       "offdays_ill_1.0                0.0517      0.284      0.182      0.855      -0.504       0.608\n",
       "healthinsurance_1.0            0.4850      0.175      2.771      0.006       0.142       0.828\n",
       "speak_other_languaje_1.0      -0.1868      0.447     -0.418      0.676      -1.063       0.690\n",
       "lives_w_mother_1.0             0.2421      0.206      1.176      0.240      -0.161       0.646\n",
       "lives_w_father_1.0             0.0442      0.248      0.178      0.859      -0.442       0.530\n",
       "read_overall_1.0               0.1261      0.212      0.596      0.551      -0.289       0.541\n",
       "read_overall_2.0               0.3984      0.239      1.666      0.096      -0.070       0.867\n",
       "read_overall_3.0               0.2478      0.243      1.020      0.308      -0.228       0.724\n",
       "write_overall_1.0              0.0905      0.175      0.517      0.605      -0.252       0.433\n",
       "write_overall_2.0              0.3762      0.277      1.360      0.174      -0.166       0.918\n",
       "write_overall_3.0              0.4217      0.356      1.184      0.237      -0.277       1.120\n",
       "numeracy_overall_1.0           0.1176      0.249      0.472      0.637      -0.371       0.606\n",
       "numeracy_overall_2.0          -0.0822      0.274     -0.300      0.764      -0.620       0.455\n",
       "numeracy_overall_3.0           0.1395      0.338      0.413      0.680      -0.523       0.802\n",
       "supervise_1.0                  0.0888      0.179      0.495      0.621      -0.263       0.440\n",
       "computer_use_overall_1.0       0.2742      0.267      1.027      0.305      -0.249       0.798\n",
       "computer_use_overall_2.0      -0.2644      0.246     -1.075      0.283      -0.746       0.218\n",
       "computer_use_overall_3.0      -0.1024      0.215     -0.476      0.634      -0.524       0.319\n",
       "think_learn_work_1.0           0.1372      0.224      0.611      0.541      -0.303       0.577\n",
       "think_learn_work_2.0           0.0733      0.210      0.350      0.727      -0.338       0.484\n",
       "think_learn_work_3.0           0.1505      0.242      0.621      0.535      -0.325       0.626\n",
       "autonomy_at_work_1.0          -0.4567      0.304     -1.502      0.133      -1.053       0.139\n",
       "autonomy_at_work_2.0          -0.3696      0.296     -1.247      0.212      -0.950       0.211\n",
       "autonomy_at_work_3.0          -0.1993      0.298     -0.669      0.504      -0.783       0.385\n",
       "repetitiveness_at_work_1.0     0.1006      0.251      0.401      0.689      -0.391       0.593\n",
       "repetitiveness_at_work_2.0     0.0672      0.321      0.209      0.834      -0.562       0.696\n",
       "repetitiveness_at_work_3.0     0.2068      0.397      0.521      0.602      -0.571       0.985\n",
       "physical_demand_work_1.0      -0.3446      0.303     -1.137      0.255      -0.939       0.249\n",
       "physical_demand_work_2.0      -0.2605      0.303     -0.859      0.390      -0.854       0.334\n",
       "physical_demand_work_3.0      -0.3868      0.293     -1.320      0.187      -0.961       0.187\n",
       "has_children_1.0              -0.3143      0.168     -1.876      0.061      -0.643       0.014\n",
       "hh_size_2                     -0.1484      0.158     -0.938      0.348      -0.458       0.161\n",
       "gender_1                      -0.0578      0.159     -0.364      0.716      -0.369       0.254\n",
       "has_spouse_1                   0.4047      0.156      2.586      0.010       0.098       0.711\n",
       "chronic_disease_1.0           -0.3579      0.178     -2.009      0.044      -0.707      -0.009\n",
       "shocks_bef_15_1.0             -0.1890      0.138     -1.373      0.170      -0.459       0.081\n",
       "mother_tongue_1.0              0.4931      1.286      0.384      0.701      -2.027       3.013\n",
       "labor_market_status_1.0        0.4203      0.264      1.593      0.111      -0.097       0.937\n",
       "job_stable_1.0                 0.3112      0.169      1.845      0.065      -0.019       0.642\n",
       "highest_ISCED_PIAAC_1         -0.5252    530.321     -0.001      0.999   -1039.936    1038.885\n",
       "highest_ISCED_PIAAC_2         -1.4265   1060.642     -0.001      0.999   -2080.247    2077.394\n",
       "highest_ISCED_PIAAC_3         -1.9327   1590.963     -0.001      0.999   -3120.164    3116.298\n",
       "highest_ISCED_PIAAC_5         -2.7007   2651.606     -0.001      0.999   -5199.753    5194.351\n",
       "highest_ISCED_PIAAC_6         -3.9744   3181.927     -0.001      0.999   -6240.437    6232.488\n",
       "highest_ISCED_PIAAC_7         -5.4589   3712.248     -0.001      0.999   -7281.332    7270.414\n",
       "highest_ISCED_PIAAC_8         10.6973   8286.268      0.001      0.999   -1.62e+04    1.63e+04\n",
       "dropout_1.0                   -0.0872      0.090     -0.966      0.334      -0.264       0.090\n",
       "==============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting coefficients \n",
    "logreg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_logreg = {'ses_15': 0.1929, 'extraversion_av': -0.1406, 'conscientiousness_avg':-0.1809, \n",
    "                'openness_av': 0.1991, 'stability_av': 0.4002, 'agreeableness_av': 0.0124, \n",
    "                'grit_av': 0.0207, 'decision_av':-0.2189, 'hostile_av':-0.4082, 'risk': -0.0219, \n",
    "                'age': -0.0027, 'BMI':-0.0122, 'hour_earnings':8.801e-06, 'total_hr_worked_week': -0.0018, \n",
    "                'highest_ISCED_PIAAC': 0.8402, 'years_educ': -0.0605, 'wealth_index': 0.4449, \n",
    "                'overqualified': -0.1388, 'dropout': -0.0872, 'in_school_1.0': 0.3564, 'owns_house_2': 0.0983, \n",
    "                'owns_house_3':-0.1115, 'house_beds_2': -0.2382, 'house_beds_3': -0.2529, \n",
    "                'house_kitchen_1': -0.3204, 'reported_social_status_1': 0.6997, \n",
    "                'reported_social_status_2': 0.4566, 'reported_social_status_3': 0.6023, \n",
    "                'reported_social_status_4': 0.1483, 'reported_social_status_5': 0.5546, \n",
    "                'reported_social_status_6': -0.0432, 'got_pr_transf_1':0.0024, 'got_pu_transf_1': 0.1019, \n",
    "                'part_in_training_1.0': 0.0917, 'offdays_ill_1.0': 0.0517, 'healthinsurance_1.0': 0.4850, \n",
    "                'speak_other_languaje_1.0': -0.1868, 'lives_w_mother_1.0': 0.2421, 'lives_w_father_1.0': 0.0442, \n",
    "                'read_overall_1.0': 0.1261, 'read_overall_2.0': 0.3984, 'read_overall_3.0': 0.4217, \n",
    "                'write_overall_1.0': 0.0905, 'write_overall_2.0': 0.3762, 'write_overall_3.0': -0.0243, \n",
    "                'numeracy_overall_1.0': 0.1176, 'numeracy_overall_2.0': -0.0822, \n",
    "                'numeracy_overall_3.0': 0.1395, 'supervise_1.0': 0.0888, 'computer_use_overall_1.0': 0.2742, \n",
    "                'computer_use_overall_2.0': -0.2644, 'computer_use_overall_3.0': -0.1024, \n",
    "                'think_learn_work_1.0': 0.1372, 'think_learn_work_2.0': 0.0733, 'think_learn_work_3.0': 0.1505, \n",
    "                'autonomy_at_work_1.0': -0.4567, 'autonomy_at_work_2.0': -0.3696, 'autonomy_at_work_3.0': -0.1993,\n",
    "                'repetitiveness_at_work_1.0': 0.1006, 'repetitiveness_at_work_2.0':0.0672, \n",
    "                'repetitiveness_at_work_3.0': -0.3868, 'physical_demand_work_1.0': -0.3446, 'physical_demand_work_2.0': -0.2605, \n",
    "                'physical_demand_work_3.0': -0.0428, 'has_children_1.0': -0.3143, 'hh_size_2': -0.1484, 'gender_1': -0.0578, \n",
    "                'has_spouse_1': 0.4047, 'chronic_disease_1.0': -0.3579, 'shocks_bef_15_1.0': -0.1890, \n",
    "                'mother_tongue_1.0': 0.4931, 'labor_market_status_1.0': 0.4203, 'job_stable_1.0': 0.3112, \n",
    "                'highest_ISCED_PIAAC_1': -0.5252, 'highest_ISCED_PIAAC_2': -1.4265, 'highest_ISCED_PIAAC_3': -1.9327,\n",
    "                'highest_ISCED_PIAAC_5': -2.7007, 'highest_ISCED_PIAAC_6': -3.9744, 'highest_ISCED_PIAAC_7': -5.4589, \n",
    "                'highest_ISCED_PIAAC_8': 10.6973, 'dropout_1.0': -0.0872}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "highest_ISCED_PIAAC 0.8402\n",
      "2.316830296506876\n",
      "No impact in model\n",
      "wealth_index 0.4449\n",
      "1.5603341546152745\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "reported_social_status_1 0.6997\n",
      "2.013148672268046\n",
      "reported_social_status_2 0.4566\n",
      "1.5786972790252074\n",
      "reported_social_status_3 0.6023\n",
      "1.826314496832713\n",
      "No impact in model\n",
      "reported_social_status_5 0.5546\n",
      "1.7412443477168658\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "healthinsurance_1.0 0.485\n",
      "1.6241750088442293\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "read_overall_3.0 0.4217\n",
      "1.524551090744396\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "mother_tongue_1.0 0.4931\n",
      "1.6373842516276846\n",
      "labor_market_status_1.0 0.4203\n",
      "1.5224182125804386\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "No impact in model\n",
      "highest_ISCED_PIAAC_8 10.6973\n",
      "44236.25585312681\n",
      "No impact in model\n"
     ]
    }
   ],
   "source": [
    "# The list below shows that changes in 8 variables correspond to increases in happiness of between a 3% and 5%. Other \n",
    "# variables have a smaller impact. It is possible that the poisson regression is not the best model to identify \n",
    "\n",
    "for item, value in coefficients_logreg.items(): \n",
    "    coeff = np.exp(value)\n",
    "    if coeff >= 1.5: \n",
    "        print(item,value)\n",
    "        print(coeff)\n",
    "    else: \n",
    "        print('No impact in model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PCA for Multiclass Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating PCA \n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting on train data \n",
    "pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the train data \n",
    "z_train = pca.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking resulting data \n",
    "z_traindf = pd.DataFrame(z_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>1.962000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.920984e-16</td>\n",
       "      <td>-9.755477e-17</td>\n",
       "      <td>-5.958536e-17</td>\n",
       "      <td>9.744159e-17</td>\n",
       "      <td>3.559278e-17</td>\n",
       "      <td>1.264138e-16</td>\n",
       "      <td>-3.078294e-17</td>\n",
       "      <td>6.269761e-17</td>\n",
       "      <td>1.801707e-16</td>\n",
       "      <td>7.243045e-18</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.704549e-18</td>\n",
       "      <td>4.838128e-18</td>\n",
       "      <td>3.963870e-17</td>\n",
       "      <td>2.576091e-17</td>\n",
       "      <td>-2.750094e-17</td>\n",
       "      <td>2.659556e-17</td>\n",
       "      <td>-2.176450e-17</td>\n",
       "      <td>-4.368196e-18</td>\n",
       "      <td>2.584572e-17</td>\n",
       "      <td>1.450730e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.655520e+00</td>\n",
       "      <td>2.046610e+00</td>\n",
       "      <td>1.674898e+00</td>\n",
       "      <td>1.516893e+00</td>\n",
       "      <td>1.414982e+00</td>\n",
       "      <td>1.383593e+00</td>\n",
       "      <td>1.316781e+00</td>\n",
       "      <td>1.299379e+00</td>\n",
       "      <td>1.273867e+00</td>\n",
       "      <td>1.251801e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.750310e-01</td>\n",
       "      <td>4.134024e-01</td>\n",
       "      <td>3.593142e-01</td>\n",
       "      <td>3.239860e-01</td>\n",
       "      <td>2.770041e-01</td>\n",
       "      <td>2.673108e-01</td>\n",
       "      <td>1.525572e-01</td>\n",
       "      <td>5.454159e-02</td>\n",
       "      <td>1.205947e-15</td>\n",
       "      <td>1.188388e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.388003e+00</td>\n",
       "      <td>-4.745355e+00</td>\n",
       "      <td>-4.977375e+00</td>\n",
       "      <td>-4.373337e+00</td>\n",
       "      <td>-4.452545e+00</td>\n",
       "      <td>-4.149632e+00</td>\n",
       "      <td>-3.876473e+00</td>\n",
       "      <td>-3.362988e+00</td>\n",
       "      <td>-4.018707e+00</td>\n",
       "      <td>-3.635786e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.765213e+00</td>\n",
       "      <td>-1.576114e+00</td>\n",
       "      <td>-1.296093e+00</td>\n",
       "      <td>-8.654606e-01</td>\n",
       "      <td>-8.508406e-01</td>\n",
       "      <td>-1.004055e+00</td>\n",
       "      <td>-6.502201e-01</td>\n",
       "      <td>-5.868240e-02</td>\n",
       "      <td>-1.111154e-14</td>\n",
       "      <td>-4.618352e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.403301e+00</td>\n",
       "      <td>-1.532179e+00</td>\n",
       "      <td>-1.185430e+00</td>\n",
       "      <td>-1.076374e+00</td>\n",
       "      <td>-9.596860e-01</td>\n",
       "      <td>-9.728178e-01</td>\n",
       "      <td>-9.151996e-01</td>\n",
       "      <td>-8.665023e-01</td>\n",
       "      <td>-9.172785e-01</td>\n",
       "      <td>-7.985253e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.049931e-01</td>\n",
       "      <td>-2.490098e-01</td>\n",
       "      <td>-1.906404e-01</td>\n",
       "      <td>-1.276560e-01</td>\n",
       "      <td>-1.248854e-01</td>\n",
       "      <td>-9.652162e-02</td>\n",
       "      <td>-5.903159e-02</td>\n",
       "      <td>-1.025183e-02</td>\n",
       "      <td>-7.309500e-16</td>\n",
       "      <td>-6.770599e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-6.701541e-02</td>\n",
       "      <td>-1.884439e-01</td>\n",
       "      <td>-1.020931e-01</td>\n",
       "      <td>1.995214e-02</td>\n",
       "      <td>-2.770067e-02</td>\n",
       "      <td>-2.021905e-02</td>\n",
       "      <td>8.508707e-02</td>\n",
       "      <td>-8.598969e-02</td>\n",
       "      <td>-4.528600e-02</td>\n",
       "      <td>3.305317e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.816295e-02</td>\n",
       "      <td>-1.905634e-02</td>\n",
       "      <td>-2.314381e-02</td>\n",
       "      <td>-1.667598e-02</td>\n",
       "      <td>-2.464642e-02</td>\n",
       "      <td>-1.379998e-02</td>\n",
       "      <td>9.957611e-03</td>\n",
       "      <td>-1.905887e-03</td>\n",
       "      <td>-9.305067e-18</td>\n",
       "      <td>1.000962e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.079742e+00</td>\n",
       "      <td>1.480571e+00</td>\n",
       "      <td>1.110228e+00</td>\n",
       "      <td>1.094077e+00</td>\n",
       "      <td>9.222735e-01</td>\n",
       "      <td>9.078674e-01</td>\n",
       "      <td>9.009116e-01</td>\n",
       "      <td>7.398408e-01</td>\n",
       "      <td>9.110339e-01</td>\n",
       "      <td>8.481206e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.741770e-01</td>\n",
       "      <td>2.017292e-01</td>\n",
       "      <td>1.825524e-01</td>\n",
       "      <td>9.632361e-02</td>\n",
       "      <td>6.861876e-02</td>\n",
       "      <td>7.589179e-02</td>\n",
       "      <td>8.633054e-02</td>\n",
       "      <td>5.805401e-03</td>\n",
       "      <td>7.123399e-16</td>\n",
       "      <td>8.772523e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.765767e+00</td>\n",
       "      <td>5.384180e+00</td>\n",
       "      <td>5.447831e+00</td>\n",
       "      <td>5.276480e+00</td>\n",
       "      <td>4.535259e+00</td>\n",
       "      <td>4.870289e+00</td>\n",
       "      <td>5.378896e+00</td>\n",
       "      <td>5.673418e+00</td>\n",
       "      <td>4.121657e+00</td>\n",
       "      <td>4.041902e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.899575e+00</td>\n",
       "      <td>1.615041e+00</td>\n",
       "      <td>1.318744e+00</td>\n",
       "      <td>1.403272e+00</td>\n",
       "      <td>1.483757e+00</td>\n",
       "      <td>1.368672e+00</td>\n",
       "      <td>8.583864e-01</td>\n",
       "      <td>1.197371e+00</td>\n",
       "      <td>5.042210e-15</td>\n",
       "      <td>4.119998e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  1.962000e+03  1.962000e+03  1.962000e+03  1.962000e+03  1.962000e+03   \n",
       "mean   2.920984e-16 -9.755477e-17 -5.958536e-17  9.744159e-17  3.559278e-17   \n",
       "std    2.655520e+00  2.046610e+00  1.674898e+00  1.516893e+00  1.414982e+00   \n",
       "min   -5.388003e+00 -4.745355e+00 -4.977375e+00 -4.373337e+00 -4.452545e+00   \n",
       "25%   -2.403301e+00 -1.532179e+00 -1.185430e+00 -1.076374e+00 -9.596860e-01   \n",
       "50%   -6.701541e-02 -1.884439e-01 -1.020931e-01  1.995214e-02 -2.770067e-02   \n",
       "75%    2.079742e+00  1.480571e+00  1.110228e+00  1.094077e+00  9.222735e-01   \n",
       "max    7.765767e+00  5.384180e+00  5.447831e+00  5.276480e+00  4.535259e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  1.962000e+03  1.962000e+03  1.962000e+03  1.962000e+03  1.962000e+03   \n",
       "mean   1.264138e-16 -3.078294e-17  6.269761e-17  1.801707e-16  7.243045e-18   \n",
       "std    1.383593e+00  1.316781e+00  1.299379e+00  1.273867e+00  1.251801e+00   \n",
       "min   -4.149632e+00 -3.876473e+00 -3.362988e+00 -4.018707e+00 -3.635786e+00   \n",
       "25%   -9.728178e-01 -9.151996e-01 -8.665023e-01 -9.172785e-01 -7.985253e-01   \n",
       "50%   -2.021905e-02  8.508707e-02 -8.598969e-02 -4.528600e-02  3.305317e-02   \n",
       "75%    9.078674e-01  9.009116e-01  7.398408e-01  9.110339e-01  8.481206e-01   \n",
       "max    4.870289e+00  5.378896e+00  5.673418e+00  4.121657e+00  4.041902e+00   \n",
       "\n",
       "       ...            71            72            73            74  \\\n",
       "count  ...  1.962000e+03  1.962000e+03  1.962000e+03  1.962000e+03   \n",
       "mean   ... -9.704549e-18  4.838128e-18  3.963870e-17  2.576091e-17   \n",
       "std    ...  4.750310e-01  4.134024e-01  3.593142e-01  3.239860e-01   \n",
       "min    ... -1.765213e+00 -1.576114e+00 -1.296093e+00 -8.654606e-01   \n",
       "25%    ... -3.049931e-01 -2.490098e-01 -1.906404e-01 -1.276560e-01   \n",
       "50%    ... -3.816295e-02 -1.905634e-02 -2.314381e-02 -1.667598e-02   \n",
       "75%    ...  2.741770e-01  2.017292e-01  1.825524e-01  9.632361e-02   \n",
       "max    ...  1.899575e+00  1.615041e+00  1.318744e+00  1.403272e+00   \n",
       "\n",
       "                 75            76            77            78            79  \\\n",
       "count  1.962000e+03  1.962000e+03  1.962000e+03  1.962000e+03  1.962000e+03   \n",
       "mean  -2.750094e-17  2.659556e-17 -2.176450e-17 -4.368196e-18  2.584572e-17   \n",
       "std    2.770041e-01  2.673108e-01  1.525572e-01  5.454159e-02  1.205947e-15   \n",
       "min   -8.508406e-01 -1.004055e+00 -6.502201e-01 -5.868240e-02 -1.111154e-14   \n",
       "25%   -1.248854e-01 -9.652162e-02 -5.903159e-02 -1.025183e-02 -7.309500e-16   \n",
       "50%   -2.464642e-02 -1.379998e-02  9.957611e-03 -1.905887e-03 -9.305067e-18   \n",
       "75%    6.861876e-02  7.589179e-02  8.633054e-02  5.805401e-03  7.123399e-16   \n",
       "max    1.483757e+00  1.368672e+00  8.583864e-01  1.197371e+00  5.042210e-15   \n",
       "\n",
       "                 80  \n",
       "count  1.962000e+03  \n",
       "mean   1.450730e-16  \n",
       "std    1.188388e-15  \n",
       "min   -4.618352e-15  \n",
       "25%   -6.770599e-16  \n",
       "50%    1.000962e-16  \n",
       "75%    8.772523e-16  \n",
       "max    4.119998e-15  \n",
       "\n",
       "[8 rows x 81 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming test data \n",
    "z_test = pca.transform(X_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the proportion of explained variance\n",
    "var_exp = pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the cumulative explained variance \n",
    "cev = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAEbCAYAAAD54N9hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3hUZfbA8e9JIRAILaEXg3Qk1ACiglhpK6iLPwEVMSoilnXVtey6iKKIBdG1sy5VWHsBBBvCAiIKERsldCU0IbQACckk7++P9yaZmUySCSQzKefzPPNk7nvbmTvJzMnbrhhjUEoppZQqy0KCHYBSSimlVFE0YVFKKaVUmacJi1JKKaXKPE1YlFJKKVXmacKilFJKqTJPExallFJKlXmasKigEpHeIvKuiOwRkQwRSRGRL0XkRhEJDXZ85ZGIjBaRhGDH4U1E7hERIyI9C9lmhYjsEBEpoXO+JSJbS+JYwSAirZxrVtCjYymd9xbn+E2LuV+Ys98jpRFXEecu1++1KlpYsANQlZeI3AM8D3wNPAj8BtQBLgdeA44AnwQtwPJrNPZve3qQ4/A2D3gWuAH43nuliLQAzgcmmpKbIOpRIKqEjhVMTwCf+ijfFuhACmOMcYlIb2BXsGNRFY8mLCooRKQvNll52Rhzt9fqT0TkeaB64CNTpcUY84eILAaGi8i9xphMr01GAQLMPtNziUiEMeaUMaZMfaGfgW3GmNXBDsIf5SVOVf5ok5AKlgeBQ8ADvlYaY7YZY37OWRaRniLylYgcF5ETIrLEu2lBRGaKSLKIxIvIKhFJE5EkERnsrL9XRHaKyDER+URE6nntb0TkSRH5h3OcNBFZLiJdvLYTEfmrc+wMEdkrIi+LSE0fx3tCRO52mjlSReR/InKO9+sVkatFZLWInBSRIyLynog099pmp1PtPVxENjrXYa2IXOC2zTLgQuB8t2aDZb6usYg0FBGXiHgnjIjIAyKSmXONRKS/c02POu9BkoiM93XcIswCYoCBPtbdAHyTk2SISDUReVFE1juvda+IzBeRtl6x5jRfnC8iH4jIUeAbZ12+ZgLnPVnn/B4cLOB36VLnmINF5DWxTZUHRGS2iNTy2jZMRP7uvCennO0Wi0gbt23qi8gbktf0uVFEbj6N6+eTiDzlnLubW1mUiGwRkZXiNK8612OniPRxfnfSnd/NcX6c4zoRWea8vlQR+UFErvdxLTyahJzrbUSkpXNdTjgxPCIiIV77+3WdRORyEfnRec1bReTW07luqpwxxuhDHwF9AKHASWCen9t3AtKARGAY8GdgjVPW2W27mcAxYAOQAAwAVgDpwBRgATDYWXcMeNfrPAZblf0NcCVwLZAEpAB13bab5Gz7MtAf+Ctw3DlXiNfxdgKfA0Oc2HcAW4Ewt+3GOttOBwY5593obBvltt1ObLPZGudYfwLWYZvOajvbdAB+AH4CznUeHQq5tp8B3/so/wVY4Dw/GzgFzHWu6cXAbcDTp/HeV3Gu53te5ec51+BWt7K6wDTnelwIXA0swSa69d22u8XZ93dgMnAp0N9Z9xaw1etc04HrgX7ONXzPeX0d3La51DnmduBFbDPlX5zfpf94He9jIBN4xrk+VwIvAH2d9bWBLc77d4tz7OeBbOD2Iq5XKyeOBGyNuPsj1G27cOA7YBNQ3SmbAxwGznLb7i3gKPb3/A4n3jnOOa73cU2bupX9ExjnXItLsc1ULuAWt23CnP0ecSt7win7GbjX2fdlp+wGt+38uk5ARyAD+/d2JTAc+3e6y/u91kfFegQ9AH1UvgfQwPmwesrP7d/H7UvZKauJ/eL60K1spnPcvm5lnZyyJK8P+OedLxn3MgMczPnAd8pine0mOst1sV9uM71ivN7Zf4jX8bYA4W5lw5zy85zlGs4XyHSv47VwPpTvcSvb6XwB1XEri3eON9KtbBmw0s9re52zf1u3si5O2f95xVyzhN7/V7Bf/O7v5+vYBLRWIfuFYpsJTwJ3uZXnfLk+62OffAmLj2OGY/uCTHErz0lYvJOT14ETbsuXO9uNK+QcjzmvraVX+Qxgv/vvoI99cxIWX48jXtu2xCbi/8E2rxngGh/XwwDDvMqXAjt8XNOmBcQVgk1OZgCJbuWFJSw3eB1jI7CouNcJeAf4A4j08XeqCUsFfmiTkCoP+gILjTFHcgqMMceA+dj/vN2dMMYsd1ve5Pz8yhiT5VUeBjTy2n+RMeaE23l2AquB3k7Rudhagre89nsb+9+mdzxfGs++Gr84P3Oae3pjk6+5TnV6mIiEYf9b3OS8dnffGmMOF3K84voIWzt0g1vZDdgkar6z/CP2y+BtERkmIvVP81w5ZgERwP+B7W/iPP/EGHPUfUOn+et7p5nH5cRaDWhLfh/5c3KnOWGZiKQ4x8zA1iL5OqZ3R9dfgEgRiXGWLweysElCQQYAq4DfvN7jz4H6BZzX22NAD69HP/cNjG1KG4etjZmGTYLf83GsTPJfq7eBWBFpWFAAItJWRN4Rkd3Y65aJ7eDtT/yQ/1r+iufvrb/XqTf28+Bkzo5uf6eqAtOERQVDCvY/qbP83L4usNdH+T7sqCJ3R9wXjDEZztPDXtvllFf1Kt/v4zz7gSZuseAdjzHGhdN05LXvIa/lU17nzfny/wr7BeD+iAOiCzueMcb7eMXifOh/AFwnVigwAttkk+5ssxXb9BWCbT7YJ7a/jXdy5u85v8cmY6Ocoiuw7+Ms9+1E5Crgv9gvthFAL+wX9SF8v15fvyMenL4qn2ITsgRsAtrDOYevYxb1/kUDB93eB1/qY5vRvN/f/7odoyg7jTFrvR4/+thuPvZvIAKYWsCxUrySd8j7vW/ivTGA2P5ZXwLnYPufXYC9brPw83fPGOPrWrrv6+91akTBf6eqAtNRQirgjB36uAy4TJzRHEXscgjw9Z9fQ/InImeqQQFlu91iyTn3+pwNnP8Eo8n/BVeUFOfnaPfjuUkt5vFOxxzgRuyXUDXsF8Ic9w2MMUuBpU5tyPnA48CnIhJrjDl4GuecBTwlImdja3T2AV94bTMc2GSMyZ1TRkSqYvs6+GL8OO+fsc1Rf3aSzJzj1uX0vvAOAjFF/B6nYGvM7i1gfdJpnLcgr2P7fOwApolIX/fX6YgWkVCvpCXn9343vp0PNAN6G7dRQCISXkJxg//XaS8F/52qCkxrWFSwTMZ+wT/ja6WItBCRTs7i/4BBIhLltj4K+5/5shKOa5CI5A6nFpFY7H/h3zpFq7G1M8O99rsW+w9AceNZhU1KWvn4D3qtMeZ0vsxOYRMPfy0FkrGJww3YvjIrfG1o7FDhr7HvW3VsX5vT8Rb2i/Ue7IihuT7+64/ENj24G8WZfW7lHDM3uRGRy4HGp3m8L7D9YAqbqO8zoD2+a0nWGmNKJCkVkRuxNVG3YH8/e2DnofEWDlzlVTYc24dlXwGHj3R+5jZvOs1iV5xJzF78vU7fAn8SkZyY3P9OVQWmNSwqKIwxy0XkXuB5EemA7TD7O7Zp4BLsh+5I7MiCidjRHEtE5Gnsl82D2A/Rx0s4tDTgCxF5Flut/hi2I+NUJ+5DIjIFeFhETgCLsB+yTwAr8T25V4GMMcdE5G/AK2KHEC/GNlc0wfaHWWaMmVfM17ABGCci12I7k6YWlvgYY7JFZC525E84MNUY4/6FPhbbl2YR9j/gGOBhYA+2KQWneWgJkGCMKXIeFWNMsogsAe7Ezr0yy8dmnwEvi8hz2OvSw9n+WFHHL8RnzjFmiMgsoB3wiPNais0Y86WIfAy8KCJnYZO/Ktj+JR8bY1YAzwHXACtEZCqwGTuZXTts52vv5MGXliLi6ws5yRhzWERaYUfevGGM+QhA7LDzJ0TkCyeOHMeAqU5fpO3Yjtf9sB3HC/INtv/QayIywYn/n9jOr7F+xO8Pf6/TRGzC9bnzu1ENmICtpVMVWbB7/eqjcj+ww1nfw1bzZmKbVL7Afni6DxHuhe3ncRw4gf1y7Ol1rJlAso9zGOAJr7LRTnkrr+2eBP6OrXFIx9Y0dPHaV7BDmZOwtS17sSNfavpx3linfLRX+SDsl90x7CiYLdjht+5DbXcCbxXw+ia4LTfEJhepzrplfrwP55A3+qSN17re2BmHd2Frb/Y675n7yKJ+vl5XEefMGaG0roD1odgh5Huca7IU6Oy8N2+6bZczoiXWxzF8DWu+x7mWadgZdy/CJptfuW2TM0qon9e+vob7hmO/vLc4vw8HgIVAa7dt6mKHR+90tvkDWI7baKcCrkFho4QMdlhvuPM6NgDV3PYNwc4i/TvOyDLneuzENv+tdd7PncAdfrzOS7EdsNOxQ/PvxBna7LZNgaOE/Hxv/LpO2D5VPzrxb3PiLXREmD7K/0OcN1+pSk9EDPCkMSbg90FRKhBE5C3gAmNMbLBjUaq4tA+LUkoppco8TViUUkopVeZpk5BSSimlyjytYVFKKaVUmVeuhzXHxMSY2NjYYIehlFJKqRKQmJh40BhTz9e6cp2wxMbGsnbt2mCHoZRSSqkSICK/FbROm4SUUkopVeZpwqKUUkqpMk8TFqWUUkqVeZqwKKWUUqrMK9edbguSmZlJcnIy6enpwQ5FqXKtatWqNG3alPDw8GCHopSq5AKSsIjIdOzddv8wxnT0sV6wN7wahL3J2WhjzA+ne77k5GSioqKIjY3FHlopVVzGGFJSUkhOTqZFixbBDkcpVckFqoZlJvbW5wXddn4g0Np59AJec36elvT0dE1WlDpDIkJ0dDQHDhwIdihKqYKYbMhKh+xT9qf7w2Q5j+y852S77XsGM91LKDS48IzDL46AJCzGmOUiElvIJkOB2cbeJ2C1iNQWkUbGmL2ne05NVpQ6c/p3pFQJyzoFp1Ig45D9mXkUMo/lf7iOg+uE288TkHXCLSFJcxKVjOC8jrAa8H+pgT1lQM9WsCbALrflZKcsX8IiImOAMQDNmzcPSHBKKaVUgbJdkL4PTu6BtD2QvhfS9kLaPvszfR+k/wEZKTbxUKelrCQsfjPGTAOmAcTHx5f5OzcOHDiQoUOHMnbs2NwyYwwtW7ZkxowZXHih/1Vqe/bs4brrrmPp0qWlEarfRo8ezVdffUVMTExu2SOPPMKwYcNO63jLli3j/vvvL3LW4vnz57NixQqeffbZ0zqPLzNnzmThwoW8//77JXZMpVQFYgycOgDHd8LJ3+BEzuN3SNsNJ3dD+n4giF9HodUgJAJCqzqPCLssYbbpRkJBQvJ+4lZzerq1qKHVSiT04igrCctuoJnbclOnrNxLSEhgypQpHgnLsmXLCAkJoW/fvn4fx+Vy0bhx46AnKzkeeugh7rzzzoCec8iQIQwZMiSg51RKVQKuE3B8O6RuheM74MQOz59ZaSV3LgmFiGioUtd51IHwmhBey/lZE8KjICwKwqrbppew6nmP0GpOUuL8DKly+klHOVNWEpb5wJ0i8ja2s+3RM+m/4i72oU9L4jCF2jl5cIHrhg4dyu23387GjRtp3749ADNmzOCmm25CRFiyZAmPPPII6enpuFwu/vGPfzB8+HAA+vXrR5cuXVi9ejV169bl1VdfJT4+noMHDwJw3XXXkZSUxKlTp2jVqhXTp0+nTp06LFu2jHvuuYdevXrx7bffIiK8/fbbueefPn06L774IgBVqlRh4cKFNGjQgEWLFvHkk0+Snp5OlSpVmDp1Kueee67f1yEtLY1evXoxceJEhg4dytdff824ceNYs2YNiYmJ/OUvf6Fz584kJiZSvXp1Zs6cSYcOHTyO4XK5GDx4MCkpKaSlpdGzZ0/eeOMNqlSp4lEbUtRrnDVrFq+++ioul4tatWrx2muv0bZtWzIyMrjrrrv4+uuviYmJoWvXrn6/PqVUOZaVbhOSY0mQutlJTrban2l7zvz4VetDtSZQrTFUa2QfVRu6Pa8PVaJtQlJJEoySFqhhzf8F+gExIpIMPAqEAxhjXgcWYYc0b8UOa74pEHEFQpUqVbjuuuuYMWMGzzzzDKmpqXz88cds2LABgG7durFy5UpCQ0PZv38/3bt3p3///tSpUweA7du3s3LlSsLCwti5c6fHsV988cXcZplHHnmEp59+msmTJwOwfv16ZsyYwRtvvMGTTz7JE088wdy5c1m2bBmTJk1i5cqVNGzYkOPHjxMWFsa2bduYOHEin3/+OTVr1mT9+vUMHDiQ33//3efrmjx5Mm+++Wbu8syZM+nSpQvvvvsul19+OY0aNeLmm2/mww8/JCoqCoCff/6Zf/3rX8yePZtZs2YxatSofM1AoaGhzJs3j+joaIwx3HjjjUyfPt2jhipHQa9xxYoVvPvuuyxfvpyIiAgWL15MQkIC33zzDW+88QY7duxgw4YNZGZm0rdvX/SO30pVEDnNN0c3wrENzk8nQTnxG6fdbBNeC6qflf8R2SwvQQnRuYpKW6BGCY0oYr0B7ghELMGQkJDAgAEDeOqpp3jnnXc4//zzadq0KQAHDhwgISGBLVu2EBYWxqFDh0hKSsqt2Rg5ciRhYb7fptmzZzN37lwyMjI4ceIEbdq0yV3Xtm3b3NqDc889lwULFgDw6aefMmrUKBo2bAhAjRo1APj888/Ztm2bRzOVy+Vi//79NGjQIN+5C2oSateuHY8//jjnnXceU6dO9ajBaNWqVW6fnRtuuIExY8Zw7Ngxj/2zs7N57rnnWLx4MVlZWRw+fJjIyEifr7+g17hgwQJ++uknevWyI+ONMRw+fBiApUuXcuONNxIeHk54eDjXX389K1eu9Hl8pVQZlnEYjvyS9zi6Ho5usKNvikvCoEYLqNESapxtn1dv4ZS1sM02KujKSpNQqSmsuSZQOnfuTOPGjVm8eDEzZszgnnvuyV13++23M2TIED788ENEhDZt2njM0JuTUHhbsWIFr732GqtWraJevXrMmzePadOm5a6vWrVq7vPQ0FBcLlehMRpjGDBgALNnFzRVjv9++OEH6tWrR3JycrH3nTdvHitXrmTFihVERUUxadIkNm/e7HPbgl6jMYaEhAQef/zx03sBSqmyIzvLNt0c/hEOr4PDP9kEJa2Y3RwlBKrHQlQb+6jZBmq0gqhWtrYkpMJ/HZZ7ei+hAElISGDChAls3ryZoUOH5pYfOXIkd5K7L7/8kq1bt/p1vCNHjlCrVi2io6M5deoU06dP92u/wYMHM3v2bPbv3w/A8ePHSU9P5/LLL+ezzz5j/fr1uduuWbOmGK/Q+uijj1ixYgXr169n4cKFLF68OHfdtm3bWLFiBWATk7i4OGrWrJnvdcXExBAVFcXRo0eZN29esWO44oormD17dm7ClJWVRWJiIgAXX3wxc+bMweVykZaWdlrHV0qVkqwMOLQOtv0H1twBX5wH79eChe3gm+Gw4WnY+1nhyUpYdajbHWJvgM6ToM+HMHg9/N9JGLINLloM8S9CmzugcX+IaqnJSjmh71KAjBw5kvvvv58xY8ZQpUqV3PLJkyczbtw4Hn30UXr06EGnTp38Ot6AAQN46623aNOmDTExMfTt25fvv/++yP369evHww8/zKWXXkpISAgREREsWLCA1q1b89Zbb3HzzTeTlpZGRkYG559/Pj169PB5HO8+LGPHjmXAgAHcfffdLFmyhLp16/LOO+8waNAgVq1aBUBcXBxvvvkmt99+O5GRkT5rc0aNGsUnn3xCu3btqF+/Pn369CEtrXg99Pv27cuTTz7JkCFDyMrKIiMjg2uuuYbu3bszZswYfv75Z9q3b09MTAw9evTITd6UUgGUlQFHf4GUNXAoEQ79YJezM/3bPyQCanWA2nFQqyPU7miXI5s5Q3dVRSPmTKbmDbL4+Hjja+4O9xE5qmzwd64VVfbo35M6YyYbjm2ClO9tgpKyBo785P8srVUbQp2uUKeLfdTuZJtytGakwhGRRGNMvK91+m4rpZQqWSf3QMp3ToLyHaSsBZef07jXOBvqdIO63fKSlGoNSzdeVS5owqICol+/flq7olRFlJ1pO8IeXAUHVsHBb+Gk7+kQ8qneAqJ7QN142++kblcdkaMKpAmLUkop/2Wm2qTkjxVwYIWtRfFnJtiqDSC6p33U7WETlKoxRe+nlEMTFqWUUgU7lQJ/LLePAyvs8GKTVfg+oZG25iQnQYnu6XSG1Rle1enThEUppVSe9D+cBOV/sH8ZHP216H2qx0JMb4g5D+qdZzvFaodYVcL0N0oppSqzUylOcrLUPo6uL2IHsQlJ/T5Qrw/UuwAiGwckVFW5acKilFKVieuErUHZ9xXs/9p2mC3sHjsSZjvFNrgQ6vW1NShVagcsXKVy6Ow6AZCZmcn48eNp06YNnTp1omvXrtx3331kZvo5QdIZio2N5ddfi67WnTBhAhkZefMijB8/nnfeeadEYnjnnXd83vl5woQJ3HRT8e91ecstt+TOmhssy5YtIzIyki5duuQ+Ro0adUbH9Pe96tKlS7En1CuKiHD8+PESPaYqA7JdcPA7+PUJ+KofvF8Hlg2CTc/b/ijeyUpIONQ7H875B1z0BVxzBPp/C10mQ5NBmqyooNEalgC46aabSEtLIzExkaioKFwuF9OnT+fUqVOEh5edO3w+9thj3H///bkz8ZbkvXiuvPJKxo0bx6ZNm2jXrh1g7/kza9asYt+/KCsry2OW3WDq0KFDUIZr//jjjwE/pypHju+EfV/A3i9g3xLIPFLwthJqR+00uMg+6p0PYb5vOKpUMFX8GpZ5UvqPQmzZsoWPPvqIN998k6ioKADCwsIYM2YMNWrUYMKECdx///2527svT5gwgeHDhzNo0CBatWrFtddey7p167j44otp2bIlf/vb33L38/7PvKD/1KdMmUKPHj3o2rUrvXv3zv3iu+MOe7Ps8847jy5dunDkyBFGjx7Nyy+/zMmTJ4mJieHgwYO5x7n//vt57LHHAPjuu++46KKL6N69O927d+fTTz/Nd96IiAhGjhzJjBkzcsuWLl1KeHg4ffr0weVy0b9/f+Lj4znnnHO46aabcmt7Zs6cyaWXXspVV11Fx44d+eWXX+jXrx8LFy60b/G8efTq1YuuXbvStWtXlixZ4nEdxo8fT+/evYmNjeXll1/OXbdx40Yuv/xyOnXqRFxcHLNmzQJg7969DBs2jJ49exIXF8ekSZMKeYd9e+KJJ7j66qsBOHnyJHFxcSxatCg3poceeoju3bvTqlUrj5jcFfRegWdtSGGvMSkpiYEDB9KjRw86d+7scf0//PBD2rVrR5cuXZg4cWKxX6MqQzKPQ/ICWHsXLGgL81vA97fBrg98Jyu146DtX+HChTDskFODMgkaXabJiiqztIallK1bt47WrVtTp87pTYaUmJjI2rVrqVGjBt26deOhhx5i8eLFuFwuWrRowZgxY2jdurXfxxs1ahT33XcfAF999RVjx45l9erVvPLKK7z66qusWrUq3x2iIyMjufLKK5k3bx533303LpeLefPmsWrVKo4cOcLYsWNZtGgRjRo1Yu/evfTo0YNff/2V2rU9q44TEhIYPHgwkyZNIjQ0lBkzZuQ2B4WGhjJv3jyio6MxxnDjjTcyffp0xo4dC8Dq1av56aefaNmyZb7X1L9/f0aMGIGIkJSUxCWXXOJxp+iTJ0/y7bffsnPnTjp27Mjo0aOpWrUqQ4cO5cknn+Saa64BICUlJfca/fOf/6Rv375kZGRwySWX0KNHDy677LJ8596wYQNdunTJXb766qsZP348f//73xkwYAAvvfQS69atY+DAgQwaNCh3uz/++IPExET2799P165d6du3b777SBX0XvlS0GscOXIkc+fOpV27dqSmphIfH0/v3r2pU6cOt956K6tWraJt27Y888wzPo+ryihj7NT2ez+HPZ/BwW8KvwdPtcbQ8DLncYnOHKvKJU1Yyrj+/ftTq1YtADp16kTnzp2JiIggIiKCtm3bsm3btmIlLImJiUyaNIlDhw4REhLC5s2b/dpv9OjR3H333dx9990sXryYdu3aERsby6JFi9ixYwcDBw7M3VZE2Lp1K/HxnreD6Nq1Kw0aNOCzzz6jT58+fPLJJzz99NMAZGdn89xzz7F48WKysrI4fPgwkZF5/+ldcMEFPpMVsHeBHjFiBLt37yY8PJx9+/axb98+Gja0H8rDhw8HbE1EnTp1SE5OJisrC5fLlZusAERHR3PixAmWLVvGgQMHcstTU1PZuHGjz4SloCahkJAQ3nrrLbp06ULz5s1ZuXKlx/qbb74ZgAYNGjB48GCWLVuWL2Epznvl6zVmZ2ezcePG3HUAp06dYuPGjYSGhtKtWzfatm0LwJgxY3jwwQcLPL4qAzKP2SaePZ/CnsWQXshNO0OrQf1+0Ohy+6jZXudAUeVexU9YRgb35o5du3Zly5YtHD582GctS1hYGNnZ2bnL6enpHuurVq2a+zw0NDTfssvl8us4ABkZGQwbNozly5fTrVs39uzZQ5MmTfx6HRdccAGpqan88ssvzJw5M7dmxBhDp06dWL58uV/HSUhIYMaMGezZs4c+ffrQuLEdDjlv3jxWrlzJihUriIqKYtKkSR5f0N61Pu5GjBjBlClTuPLKK8nOziYyMtLj9fu6ZlLAh3d2djYiwpo1a864f9GOHTsICQnhyJEjpKWl5TYJ+qO471VBrzEmJsZnf5f58+cX78Wo4DiWBLs/hT0L7cyyxlXwtrU7QaP+9lHvfAitWvC2SpVDFb8PS5C1bt2aIUOGcNttt5Gaam/+ldNp9Pjx47Rq1YrExESys7NJTU3N7ZdRXK1atWLNmjUALFmyhP378//3lZ6ejsvlolmzZgC8+uqrHuujoqI4evRogee48cYbmTJlCsuXL+fPf/4zYPu8bNmyhaVLl+Zut2bNGgq6C/h1113HF198wUsvvURCQkJu+ZEjR4iJicmNYd68eX6+crtvixYtAHI7Mxelbdu2hIWF8d577+WWpaSkEBUVRZ8+fZg8eXJu+a5du9i3b5/f8QAcPnyY6667jrfffptrr72WW2+91WP9zJkzAThw4ACLFi3ioosu8lhf1Hvlj7Zt2xIZGcmcOXNyyzZt2sSxY8c499xzWbduHVu2bAEoM52YK73sTNj3NSTeC2gUZEcAACAASURBVAvawMJ2sO4+Oz+Kd7ISEQ1njYBzZ8JVe2DQT9D1Gdvko8mKqoA0YQmAWbNm0bp1a7p3707Hjh2Ji4tj06ZNREREcPXVV1O3bl3at2/P1VdfTffu3U/rHBMnTmTKlCl06dKFTz/9lObNm+fbpmbNmjz++OP06NGD7t27U716dY/19913HxdffHFup1tvo0aNYs6cOQwdOjS3uaZOnTrMnz+fxx57jM6dO9O+fXsmTJhQYMJSt25dBg4cyN69exkyZIjHsVNTU2nXrh1XXHEFffr08fu1v/DCC1x55ZV069aN7du3Ex0dXeQ+YWFhfPLJJ7z++uvExcXRuXPn3E6xc+fOZcOGDcTFxREXF8e1117r83pAXh+WnEdOP5WEhAQSEhK44IILGD9+PPv27eP111/P3S8mJobu3bvTu3dvHn74YeLi4jyOW9R75Y+wsDAWLFjA22+/TadOnTjnnHMYN24cGRkZ1K9fn2nTpnHFFVfQtWtXnzVyKkBOHYIdb8HKa+GDGPj6EkiaCqlb8m9bpxt0/Cdcvhqu2g/nz4Ozb4RqjQIft1IBJgV9sZQH8fHxxlf/gY0bN9K+ffsgRKRU0WJjY1m4cCEdO3YMdih+0b+nUpC6FZLnw+75cGBlwffmCasODS+Fxn+CxoN0RllV4YlIojEm3te6it+HRSmlgi07y97VePcnNlE5trHgbSObQ5MroMmfoEE/bd5RyqEJi1IBtnPnzmCHoALBddJOf797PuxeYG8q6JNAdC9oeoVNVGp11BE9SvlQYRMWY0yBI0GUUv4pz03GQXHqEOxeCMkf2TlSsgq4fUJoNTsnStMh0HiwzouilB8qZMJStWpVUlJSiI6O1qRFqdNkjCElJcVjyLTy4WQy7PrYJil//K/g/ihV6ztNPUNsvxSdUVapYqmQCUvTpk1JTk72mPxLKVV8VatWpWnTpsEOo+w5tgWSP4RdH9q+KQWp2Q6aDoUmQyG6J4SEBi5GpSqYCpmwhIeH587LoZRSZ8wYOPKLvTfPrg/haCF31I7uBc2ugqZXQs22gYtRqQquQiYsSil1xnKSlN/fhd/fg9QCbo0gYXY0T7OrbXNPpH+zRyulikcTFqWUyuFvkhJa1U6B3/RqO/w4om5g41SqEtKERSmljm6E396B39+BY5t8bxNW3U7g1nwYNBoA4QXf30opVfI0YVFKVU7Ht8PO/8JvbxfcJyWsuh3Z0/waaDQQwqoFNkalVC5NWJRSlUfaftvcs3MepKz2vU1opJ0fRZMUpcqUgCUsIjIAeBEIBd40xkz2Wt8cmAXUdrZ5yBizKFDxKaUqqMxU2PUR7JwL+78Ck51/m9CqdgK3s661P3WOFKXKnIAkLCISCrwCXAYkA2tEZL4xZoPbZo8A7xpjXhORDsAiIDYQ8SmlKpjsTNj7Jex8C5I/9j3jrITZviixI+zoHu2TolSZFqgalp7AVmPMdgAReRsYCrgnLAao6TyvBewJUGxKqYrAGEhZY5OU396GUwVMHFmvD8ReZzvPRkQHNkal1GkLVMLSBNjltpwM9PLaZgLwhYjcBVQHLvV1IBEZA4wBaN68eYkHqpQqZ45vhx1zbaJS0DDkWh1tkhI7Eqrr54ZS5VFZ6nQ7AphpjJkiIr2BOSLS0RjPBmdjzDRgGkB8fLzemU2pyijjsB2GvPMtOPCN722qNbEJSuz1UKdTYONTSpW4QCUsu4FmbstNnTJ3NwMDAIwx34pIVSAGKOie7EqpyiTbZe+AvGMWJH8C2Rn5twmLsk09sddD/Qv13j1KVSAFJiwiEuLPAbxrQAqwBmgtIi2wicpwYKTXNr8DlwAzRaQ9UBXQuxcqVdkd+QW2z7SjfNL351+f03m2xfW286wOQ1aqQiqshsWF7QhblCL/hTHGuETkTuBzZ/vpxpj1IvI4sNYYMx+4D/i3iPzVOe9oY4w2+ShVGWUctnOlbJ8BhxJ9b1M3HlqMgrOGQ9V6gY1PKRVwhSUs7rc7HgwMA54CfgPOAh4EPvD3RM6cKou8ysa7Pd8AnO/v8ZRSFUx2FuxfAtum26HI2afyb1OtkW3uaXEj1D4n8DEqpYKmwITFGPNbznMRuReIN8YccYo2i8haYC3wWumGqJSq0E78BttmwPbpcHJX/vUhEdD0Sjh7NDS8FELK0lgBpVSg+PuXXwuIBI64lUU65UopVTxZGbB7Pmx7E/Z+gc/W57rxcPZNdmK3KnUCHqJSqmzxN2GZBXwlIi9g51NpBtztlCullH+ObYZt/4bts3xP7BYRA7E3QMuboHZc4ONTSpVZ/iYsDwBbgWuBxsBe4GXg36UUl1KqoshKh98/sInKH//zsYFAw8ug1S3QZCiEVgl4iEqpss+vhMUZuvy681BKqaId3QBbp8GOOZBxKP/6yKZwdoJt9qkRG/DwlFLli18Ji4gIcAt2/pR6xphOItIXaGiMebc0A1RKlSOuNNj1Pmx9w/cMtBIKTf4ELW+1c6foxG5KKT/52yT0OPZOyy+QV8uSDEwFNGFRqrI7st6pTZkNmUfyr6/ewjb5tBgNkY0DHp5SqvzzN2EZDXQ1xhwUkZxhzDuAs0slKqVU2edKg9/fs7UpB1flXy9hdjhyq1vtcGT/Js9WSimf/E1YQoHjzvOc8Yc13MqUUpVFUbUpNc6GVmNsbUq1BgEPTylVMfmbsCwCnnemzc/p0zIRWFBagSmlypCsDNj1IWx5FQ6syL8+pzal9W3Q4GKtTVFKlTh/E5Z7sXOuHAXCsTUrXwCjSikupVRZcOI32PIGbP8PpPu4cXqNs20H2rNv0toUpVSp8ndY8zHgKhGpj72P0C5jzL5SjUwpFRwm284+u+VV2L2QfLPQShg0HQqtboOGl2htilIqIE7nphwpQKSInA1gjNlesiEppYIi4zBsnwmbX4XjW/Ovr9bEJimtbrE3IVRKqQDydx6WAcB/AO9PKYPtkKuUKq8O/wSbX4adcyErLf/6hpdD69vt/Cl640GlVJD4++nzCraT7SxjjI9PNKVUuZKdaTvRbn4ZDqzMvz68lp2FtvXtULN14ONTSikv/iYsdYA3jDE+bqmqlCo30vbZIclbX4e0vfnX1+4Mbe6A2JEQVj3w8SmlVAH8TVj+A9wETC/FWJRSpeXgd5D0L9j1nq1dcSdh0OzP0PYuiDkPRIITo1JKFcLfhOVc4G4ReQjwGB1kjOlb4lEppc5c1in4/V1IegkOrcm/vmpDaD3WTvKmnWiVUmWcvwnLm85DKVXWpe2FLa/ZKfN9zZ0Scx60uQuaXQ2hVQIfn1JKnQZ/52GZVdqBKKXO0OEfYdNU+O2/+Zt9QiIgdoRNVOp2C058Sil1BgpMWETkBmPMHOd5QkHbGWO0X4tSwWKyYfenkDQV9i/Nv75aE2gzzs5GW7Ve4ONTSqkSUlgNywhgjvP8hgK2MWhHXKUCz5UGO2bZGpXUzfnXx/SGtvdAs6sgJDzw8SmlVAkrMGExxgxye35RYMJRShUq/Q87E+2WV+DUQc91EgrNhkG7v0JMr+DEp5RSpaTY01Y6d2rOHfdojMku0YiUUvkdS4JNz8P2WZB9ynNdeC070qfNnVC9eXDiU0qpUubv1PxNgJeBvkBtr9U6Nb9SpeXAN7DxWUieT76bEFY/C9r+FVomQHhUUMJTSqlA8beG5XXgJHAJ8D9s4jIBWFQ6YSlViZls2L0ANjwDB1flX183Htrfbyd703v7KKUqCX8/7c4DmhtjToiIMcb8JCI3A6uAf5deeEpVIlmnYMcc2PScbQLy1vhPNlGp31dno1VKVTr+JixZgMt5fkRE6gHHgCalEpVSlUnGUXtvn00vQPo+z3Uh4RB7A7S/D2p1CE58SilVBvibsHwHDAI+Aj4H3gHSgLWlFJdSFd/JPZD0Amx5HVypnuvCa9lp89vcDZGNgxOfUkqVIf4mLDcAIc7ze4D7gRrAC6URlFIV2rEk25F2x+z8M9JWa2yHJbcaA+E1gxOfUkqVQf5OzX/E7XkaMLG4JxKRAcCL2FFFbxpjJvvY5v+wnXkN8JMxZmRxz6NUmZWyBjY8Dbs+JN+In5rtoP0DEHud3t9HKaV8KGxq/sf9OYAxZnxR24hIKPAKcBmQDKwRkfnGmA1u27QGHgbON8YcFpH6/pxfqTLNGNj3FWyYDPu/zr8+pjd0eAia/AkkJP96pZRSQOE1LM1K8Dw9ga3GmO0AIvI2MBTY4LbNrcArxpjDAMYYH7eZVaqcMNmQ/DGsnwSHEvOvbzzYJir1Lwh8bEopVQ4VNjX/TSV4nibALrflZMB77vA2ACLyDbbZaIIx5jPvA4nIGGAMQPPmOqunKmOyM2HnPFujcmyT5zoJhbNGQIcHoHZccOJTSqlyyu9Zp5wmm/8DGgN7gHeNMVtKOJbWQD+gKbBcROLc+88AGGOmAdMA4uPjjfdBlAoKVxpsn24nezv5u+e60Kpw9s12DpUasUEJTymlyju/Gs1FZCSwDugEnADigB+ccn/sxrOJqalT5i4ZmG+MyTTG7AA2YxMYpcquzOOwcQrMPxvW3umZrITXtM0+Q3ZCj5c1WVFKqTPgbw3LE8AgY8zynAIR6QPMAeb5sf8aoLWItMAmKsMB72TnY2AEMENEYrBNRNv9jE+pwMo4CptfhqSpcCrFc11EjB2a3HocVPG+9ZZSSqnT4W/CEgV861W2Gqjuz87GGJeI3ImddC4UmG6MWe+MRFprjJnvrLtcRDZgZ9b9mzEmpeCjKhUEp1Ig6UVI+hdkHvVcV62xHZrc6lYIiwxOfEopVUGJMUV3AxGRh4G6wD+NMekiUg14DDhsjHmqlGMsUHx8vFm7VifbVQGQth82TYEtr4LrhOe66rG26efs0RAaEYzolFKqQhCRRGNMvK91/tawjAMaAn8RkcNAHUCAvSJye85GxhgdtqMqlpO7bUfabdMgK91zXVQbOOfvEDvS3vNHKaVUqfE3Ybm+VKNQqqw5uRvWPwXb/g3ZGZ7ranWEjo9As2EQEhqc+JRSqpLxd2r+//kqF5FwY0ymr3VKlUsn98CGp2DrtPyJSp1u0PGf0HSIzkqrlFIB5lfCIiJfAqOMMXvdyjphRwl1LqXYlAqctL2wfjJsfQOyT3mui+4FHcdD44EgEpz4lFKqkvO3SegH4CdnpM97wIPAA8DfSyswpQIibb+9IeHW1/L3UYnuBXEToFF/TVSUUirI/G0SelBEFgKzgWewM932NMZsLc3glCo16X/YzrRbXoWsNM91dXtAp8eg0QBNVJRSqozwe2p+oAVQEzuZW3WgaqlEpFRpSj8Im56DpJcg66Tnurrxtkal8SBNVJRSqozxtw/L+0BHYIAxZo2I3IG9189TxphnSzVCpUpCxhE7hX7SC+A67rmuTleIewya/EkTFaWUKqP8rWH5A+hqjEkDMMa84nTEnQNowqLKrsxUOzPtximQecRzXe3Otkal6VBNVJRSqozztw/LOB9lm0XkvJIPSakS4DoJm1+BjU/nv9dPrXNsotLsah2erJRS5UShn9Yi8i+v5Zu9Nnm3xCNS6kxkZcDmV2FBK/jxAc9kJao1nDcXBv4EzYdpsqKUUuVIUZ/Yo72WvZt/Liu5UJQ6A9lZsH02LGwLa++w86rkqB4LvabD4A3ONPo6O61SSpU3RTUJeTfsa0O/KluMgeSP4edH4OgGz3XVGtmZac++GUKrBCc+pZRSJaKohMX7Vs5F39pZqUDZvxR+fAhSvvcsj4i2d09ufQeEVQtObEoppUpUUQlLmIhcRF7Nivey1q2rwDv0A/z0d9j7uWd5WBS0vw/a/RXCawYnNqWUUqWiqITlD2C623KK1/IfJR6RUgVJ3QY//QN+f8ezPCQC2txpa1WqxgQnNqWUUqWq0ITFGBMboDiUKlj6Qfh1or3fT7bbzcElBFqMhrhHoXrzoIWnlFKq9BVnan6lAsuVBpv/BeufgsyjnuuaXgWdn4BaHYITm1JKqYDShEWVPSYbds6z/VRO7vJcV+8C6PosxJwbnNiUUkoFhSYsqmzZvxR+uB8O/+BZHtUGujyt0+grpVQlpQmLKhuOboR1D8CehZ7lEfXsNPqtboWQ8KCEppRSKvj8TlhEJBoYBDQyxjwjIo2BEGNMcqlFpyq+9D/g50dh27/BZOWVh1aFdvdChwd1iLJSSin/EhYRuRD4AFgLnA88A7QG7geuKLXoVMXlSoOkqbB+MrhS3VYItBhlO9RGNg1aeEoppcoWf2tYXgCuNcYsEZHDTtl3QM/SCUtVWCYbds51OtR6Vc41uBi6Pgd1uwYnNqWUUmWWvwlLrDFmifM8Z3r+jGLsrxTs/x+suw8OJXqW12xvR/40HqQdapVSSvnkb8KxQUT6G2Pc50K/FPilFGJSFc2xLfDj3yD5E8/yiHrQ6XFoeQuEaO6rlFKqYP5+S9wHLBSRT4FqIvIGtu/K0FKLTJV/GYfhl8dh88tgXHnl2qFWKaVUMfmVsBhjVotIJ+B67L2EdgE9dYSQ8ik7E7a8Dr9MgIxDnutir4fOT+pU+koppYrF31FCEcABY8wzbmXhIhJhjDlVatGp8mfP5/DDPXBsk2d5vQug21SIjg9OXEoppcq1ED+3+xLo7lXWHfjcx7aqMjq+A5ZfCcsGeCYr1VvABe/Bpcs1WVFKKXXa/O3DEocdxuzue6BzyYajyh3XSdjwtH1ku1W2hUVBx0eg7d22z4pSSil1BvytYTkKNPAqawCc8PdEIjJARJJEZKuIPFTIdn8WESMi+u94WWYM7PoQFraHXx/3TFbOvgmu2AIdHtBkRSmlVInwt4blA2CeiNwNbAdaAs8D7/qzs4iEAq8AlwHJwBoRmW+M2eC1XRTwF/LX5qiy5NgWSLwL9nq1CNaNh/iX9E7KSimlSpy/NSz/ADZim4FSgdVAEvB3P/fvCWw1xmw3xmQAb+N7SPRE4Gkg3c/jqkBynYSfHoFFHT2TlYgY6Plv6P+dJitKKaVKhb/DmtOBO0TkTiAGOGiMMUXs5q4Jdih0jmSgl/sGItINaGaM+VRE/lbQgURkDDAGoHlzHRobEMbYSd9+uAdO/JZXLiHQaqy970+VOsGLTymlVIVXnLs11wLaAjWcZQCMMV+faRAiEoJtYhpd1LbGmGnANID4+PjiJE3qdBzfDmvvgj2LPMuje0GPV6Fut+DEpZRSqlLxdx6W0dg+KMeBk26rDHC2H4fYDTRzW27qlOWIAjoCy5xEqCEwX0SGGGPW+hOjKmFZp2Djs7D+Schya6GLiIYuT9uOteJvi6JSSil1ZvytYXkSGGaMWXya51kDtBaRFthEZTgwMmelMeYotqkJABFZBtyvyUqQ7Psa1o6DY0luhQKtxkDnSRBRN2ihKaWUqpz8TVjCgC9O9yTGGJfT/+VzIBSYboxZLyKPA2uNMfNP99iqBKXtt3dT3jnXs7xOV+jxGsT08r2fUkopVcrEn76zInIvttlmojEmu9Sj8lN8fLxZu1YrYc6YyYZt02Hd3yDzSF55WJTtUNt6nN5NWSmlVKkTkURjjM952Pz9Fvortl/JAyKS4r7CGKNDdcqzoxvh+9vgwArP8rOGQ7fnoVqj4MSllFJKufE3Ybm+VKNQgZd1CtY/BRsm2bsr56jR0jb/NLoseLEppZRSXvydh+V/pR2ICqAD38B3N3t2qpUwaP836PhPCKsWvNiUUkopH4ozD0sXoA92NI/klBtjxpdCXKo0uE7AT/+ApH9hR6Q7os+FXtOgdlzQQlNKKaUK4+88LGOAqdiRQgOBxcDlwCelF5oqUfuXwuqb4cSOvLKwKOgyGVrdBiGhwYtNKaWUKoK/NSwPAAOMMStE5LAx5ioRGYidT0WVZZnHYN2DsPV1z/JGA6HnG1C9me/9lFJKqTLE34SlvjEmZxhJtoiEGGMWi8jcQvdSwbX3S/juFjj5e15ZeG3o/gK0GAUiBe+rlFJKlSH+JizJIhJrjNkJbAaGishBIKPUIlOnLzMV1t0PW6d5ljcdakcA6VBlpZRS5Yy/CcszQHtgJ/A48D5QBbi7dMJSp23fEjsCyP2uyhHR0P1lOOtarVVRSilVLvk7rHmm2/PFIlIHqGKMOV5agaliyjwOPz4AW17zLG96lVOr0iA4cSmllFIloMCERUTEOPP2i+S7La8LcDl9WcrMVP2V1oFvYNUNniOAqtSF+JfgrBFaq6KUUqrcK6yG5ShQ03nuwmPiDsDOxWKwNzNUwZCVAb88ChufsfcDytF0KPR4Hao1DF5sSimlVAkqLGE5x+15i9IORBXTkV/h2xvg8I95ZeG1bK1K7PVaq6KUUqpCKTBhMcbsAhCRUGAW0N8YcypQgakCmGxIehF+fBiy3d6OBhfDuTN1XhWllFIVUpGdbo0xWSLSAvDux6ICLW0vfDsK9n2VVxYSAV2ehrZ3Qb6uRkoppVTF4O833GPAayJyloiEikhIzqM0g1Nudi+ERZ08k5U63WDgD9DuL5qsKKWUqtD8nYflTefnDW5l2uk2ELLSYd0DsPklt0KBDg9B3AQIrRKsyJRSSqmA8Tdh0U63wXB0A3wzHI78kldWrQmcNwcaXBS8uJRSSqkA83fiuN+K3kqVGGNg238g8S5bw5Kj6VDo9R87c61SSilVifhbw4KIDAEuBGKwzUEAGGNGlUJclVfmMfh+LPz237yy0KrQbSq0uk2HKyullKqU/OqpKSKPAm84218DpAD9gSOlF1oldOgHWNzdM1mpdQ70Xwutx2qyopRSqtLyd2hJAnCZMeavQIbz8wogtrQCq1SMgaSX4IvecHxrXnnLW6H/91D7nIL3VUoppSoBf5uEahtjfnWeZ4hIuDHmexG5sLQCqzQyj8Pq0bDrg7yysBrQcxrEjghaWEoppVRZ4m/Csk1EzjHGrAd+BW4XkcPA4dILrRI4vgOWD/UcBVSnK5z/DtRsHby4lFJKqTLG34TlESBnaMpDwDygBjCuNIKqFPYvg5XD4FRKXlnrO6DbFAiNCFpYSimlVFlUaMIiIiHGmGxjzKKcMmPM90CrUo+sItvyGqy9G4zLLodUsXdXbnlTcONSSimlyqiiOt3uFpFnRKRjQKKp6LIy7JDlNePykpWqDeCSZZqsKKWUUoUoKmEZi53ldo2I/CAifxGRegGIq+LJOAzLBsDWN/LK6naHAWuhXu/gxaWUUkqVA4UmLMaYT4wx1wCNsPOwXAMki8h8EfmziIQHIshyL3WbHbK8f2le2Vkj4NIVENk0eHEppZRS5YRf87AYY44YY94wxlwAtAfWAlOBvaUZXIVw4Bv44lw4lpRX1ukJOG8uhFULXlxKKaVUOeLvxHEAiEgVIB7oBTQAfil8D499B4hIkohsFZGHfKy/V0Q2iMjPIrJERM4qTmxl0s7/wpKL4dRBuxwSAee/DR3/obPWKqWUUsXg79T8F4jINGA/8ASwGmhjjPHrlsEiEgq8AgwEOgAjRKSD12brgHhjTCfgfeAZ/15CGWQM/DIRVo2E7AxbFlEPLlkKZ10b3NiUUkqpcqioYc0TgOuxc7C8B/zJGPPNaZynJ7DVGLPdOe7bwFBgQ84Gxhi3Dh6sds5b/hgDiffA5n/lldVsD/0+hRotgheXUkopVY4VNXFcL+ykcR8bY9LP4DxNgF1uy8nOsQtyM7DY1woRGQOMAWjevPkZhFQKTDasvQu2vJpX1uAS6PM+VKkdvLiUUkqpcq7QhMUYMzBQgeQQkeux/WR83qfIGDMNmAYQHx9vAhha4Uw2rLkdtk7LK2t+je1cG6KDqZRSSqkz4e/U/GdqN9DMbbmpU+ZBRC4F/gFcaIw5FaDYzpzJhu9uhe3T88rOGg6950BIoC6xUkopVXEVa5TQGVgDtBaRFs5Io+HAfPcNRKQrdq6XIcaYPwIU15nLzoLVCZ7JSuz1mqwopZRSJSggCYsxxgXcCXwObATeNcasF5HHRWSIs9mz2BsqviciP4rI/AIOV3aYbFh9E+yYlVd29mg4d6YmK0oppVQJCti3qnMDxUVeZePdnl8aqFhKzM+Pws45ecstb4Geb4AEquJKKaWUqhz0m/V07ZwH65/IW9ZkRSmllCo1+u16Og5+Z/ut5GjUH3q8psmKUkopVUr0G7a4TuyC5VdCtjOIqWY7O92+9llRSimlSo0mLMXhOgHLh0L6PrtcpS5cuEAnhVNKKaVKmSYs/jLZ8O0oOLzOLksY9PkAoloFNy6llFKqEtCExV+/PgG7Psxb7vEqNOgXtHCUUkqpykQTFn+kbvMcEdT2L9Dq1uDFo5RSSlUymrD448cHITvTPo/pDV2fC248SimlVCWjCUtR/lgJuz7IW+42VUcEKaWUUgGmCUthTDasuy9v+azhENMrePEopZRSlZQmLIX57R1I+d4+D4mAzk8FNx6llFKqktKEpSBZ6fDTw3nLbf8CNWKDFo5SSilVmWnCUpCkF+HEb/Z5RAyc8/fgxqOUUkpVYpqw+JJ+ANZPyluOmwBVagUtHKWUUqqy04TFh4wfx0PmMbtQsy20GhPcgJRSSqlKThMWL7t2rCV027S8gi7PQkh48AJSSimllCYs7k6ccrHjqzsJlWwAkojHNB4c5KiUUkoppQmLm+phWZzVoAEA2Ua4d/MNvLJsW5CjUkoppZRO2eouNIKzhnzCtA/nkbLjS9ant2T9F5tp27Aml3VoEOzolFJKqUpLa1h8uGnocH6Oui13+Z6317F5f2oQI1JKKaUqN01YfAgPDeGV67rRtE41AE5kZHHLrLUcPpER5MiUUkqpykkTlgLUrV6FN2+MJ7JKKAC/HzrJnf/9AVdWdpAjU0oppSofTVgK0a5hTZ7/vy65y99sTeHhD3/hZIYriFEppZRSlY8mLEUY0LEhf720Te7ye4nJXDLlfyz8eQ/GmCBGppRSSlUemrD44a6LWzGkc+Pc5b1H07lz3jpG/vs7FHY54wAAEVhJREFUkvZpZ1yllFKqtGnC4oeQEOGFa7vwzLBORFevklv+7fYUBv1rBRPmr+enXUfIcGn/FqWUUqo0SHlu1oiPjzdr164N6DmPpmXywlebmf3tb2Rle167KmEhnNO4Jl2a1aZLs9p0bFKLJrWrUTU8NKAxKqWUUuWRiCQaY+J9rtOE5fQk7Utlwvz1fLs9pchtG9SMoGmdSJrVqUazupHUqhZO1fD/b+/cg+Qqrjv8/e7MSGglIoGMwSDJgHkFcBA2xcNxCAYcC5uHq2IHCKYIBaVKFSkbG8ev2CZQxmU7VMBlSFIY8Ygf2BhsIhOweZgQHJcJTxsEVoWHhARCEiAJiZVWO3NP/ui+Mz2j3dWCVjuzV+ernbp9u/t2n9NzZ+a3p3umK0ypVZgyqcJOtYwptSq7TK2xS98kZvTVmFx1keM4juPsWLhg2U6YGb9ctJI7nljB48vW8sJr/WPW9rTJVWb01dh5pxq1iqhmolrJYjqjVsmYXMuYXMmYVI2PmK41j2JSJaNayahkoiKRZaKSQSaFeun11YzJ8fpKJmpZRiX23X59SEu05TuO4zjOtjCSYPGf5t8GJDHv0D2Yd+geALy6YYDfLV/LYy+s5fFla3lu9RusWLeR/C1owg0DdTYM1IGNY2v0dqSSiUxBDE2qZNSiiKpV43klI5Oa9aTWUYCi5hEi/iGFc6mVrmSiVgnHaiVrE1SVTEhBlAVR1eovyxT6j/2ilr2FHXT0CTTbrcY+a1kQdFmW2JvYH9rr6DNrtZmWk/pHqN8ai/bxydQah6KNkE7bTIRkfFSbxyyOTzG2heXR58IBx3GcHsQFyxgyc9pkjj9od44/qLXv0GAjZ8XaTSxf08+yNf28uGYj6wfqbBrM2TTYYOPmBpvqDd4YqLO2f5A1/ZtZ0z+4xfqYiUAjNxoAGAP1HAa6bJDzligEUSGS6BBFRXklEWOFOGyKzlR8wpYiqeM8FYnNtqPQLCJ4mWj2WYjdliilJVZTO5o+tfptiskOX5qit1KI2pawLcaiEKGpQC5s3dKPQpRv2V9Rr31s4oil4591CvgoYmkX0dUhBWmrvVTwZnFwsmZ0NIxjNctCOhPZEHa7oHW6zbgJFknzgG8DFeBaM/tGR/lk4N+B9wKvAqeb2ZLxsm97UatkzJnZx5yZfaO+Js+N9ZvqrOnfzIaBOoONnHpu4dgw6nnO5rqxuZGzuR4eA/UGm+s5g42czY1QdzA5z3OjYa1jIzfqjS3bGKjnoSwP/YT+Qv1GnrRjRp7TbMspD2bheY1nXbXF6R1akT3aBGOWTBVnSqaQO6J8aXSVpvBqtZWK4CCm2sVSWyQxlleyLIrVcKxmIYpbrbRsKYRZJSNJF+I2RjWTyGtaJ712KPFZzTKqyZR9NWv1nQpWEYRgMWVfqygeg8h0Rse4CBZJFeBq4IPAcuAhSQvN7Kmk2nnAGjPbT9IZwDeB08fDvl4jy8T0vhrT+2rdNmXUdAqZwTxvCah4HKjnmBHqxKPFdPH5aGYYyTlG/MMsnNej2KpHIVfPcwYboa1GXrRfiCtiH0lZHvrIm31bkm71GfqnKdbqed4UevXcmrbSZm/Lr0Ye/YxjU/hOrNPILfZH9LFVp+VvHJPmeLXy2scx9ZvYtzXFZyMRoJaMc2q744xEIWSLKKozNkg0p5mrUcg01wYmEb1CBFUS8VcIsiJqlk7xQktQ1aKYqlXjFHoSmSwilkUkLr0+tDm0oJpUzfjKyQdvn0EZhvGKsBwJPGNmzwFI+hFwGpAKltOAf4zpW4CrJMkm8qrgHYgsE1lyY0/Bv+U00bAtBFGHiCpEURR+nRG7luiMx/ih1ia8muVpX1v2UwiuQmg28pa4TfsuPkTzKA6LdGf/QFOcpiKwYS1RGYRdHvvLmwI2T64pRGIhYpsPs3a/kuvaBSVRMCdj07SvfXxSMVr0X1xTiNFGbgxGu+uN+DwkY01H2832EoE7lC8tId+6xtk+mMHmRg4NYLDb1oyeqZMqpRUsewHLkvPlwFHD1TGzuqR1wEzglbSSpPnAfIA5c+ZsL3sdZ4cjXYPCMP9VOTsmNoSIKcROGkUsjvVGKK/HaeR0WrldOMXzRPQ18nZBWgi9QsR1Rg872y/6bXTkpe0W0dbCn0YSFS3K0vr5EP43I69xmj5M21sidtvtb8Rp/fBoTcc7o2fCLbo1s2uAayB8rbnL5jiO45SeYr2HC9mxpRBAg418C/HTFITDRDTzHOp53haFC22GY3F929rHOJVuiUjL20Rh0gAjT/xVs/H/ofzxEiwvArOT81kxb6g6yyVVgemExbeO4ziOUzoKIVjJfAp9NIyXRHoI2F/SPpImAWcACzvqLATOiemPAb/y9SuO4ziO48A4RVjimpS/A35J+FrzdWa2SNKlwMNmthBYAHxP0jPAawRR4ziO4ziOM35rWMzsDuCOjryvJulNwMfHyx7HcRzHcSYO479qxnEcx3Ec503igsVxHMdxnJ7HBYvjOI7jOD2PCxbHcRzHcXoeTeRvDktaDSzdTs2/jY5f2S0h7mM5cB/LgftYDtzHbeOdZrbbUAUTWrBsTyQ9bGZHdNuO7Yn7WA7cx3LgPpYD93H74VNCjuM4juP0PC5YHMdxHMfpeVywDM813TZgHHAfy4H7WA7cx3LgPm4nfA2L4ziO4zg9j0dYHMdxHMfpeVywOI7jOI7T87hg6UDSPEmLJT0j6QvdtmeskHSdpFWSnkzydpV0t6T/i8ddumnjtiBptqT7JD0laZGkT8X80vgIIGknSf8r6XfRz0ti/j6SHoz37Y8lTeq2rduCpIqkxyTdHs9L5R+ApCWSnpD0uKSHY17Z7tcZkm6R9AdJT0s6pkw+SjowPn/F43VJF5bJRwBJn47vN09Kuim+D437a9IFS4KkCnA1cBJwMHCmpIO7a9WYcQMwryPvC8C9ZrY/cG88n6jUgYvM7GDgaOCC+NyVyUeAAeB4MzsMmAvMk3Q08E3gCjPbD1gDnNdFG8eCTwFPJ+dl86/gA2Y2N/lNi7Ldr98GfmFmBwGHEZ7T0vhoZovj8zcXeC/QD/yMEvkoaS/gk8ARZnYoUAHOoAuvSRcs7RwJPGNmz5nZZuBHwGldtmlMMLP/Bl7ryD4NuDGmbwQ+Oq5GjSFmtsLMHo3p9YQ3xr0okY8AFtgQT2vxYcDxwC0xf0L7KWkW8BHg2nguSuTfVijN/SppOnAssADAzDab2VpK5GMHJwDPmtlSyudjFZgiqQr0ASvowmvSBUs7ewHLkvPlMa+s7G5mK2L6ZWD3bhozVkjaGzgceJAS+hinSx4HVgF3A88Ca82sHqtM9Pv2SuBzQB7PZ1Iu/woMuEvSI5Lmx7wy3a/7AKuB6+P03rWSplIuH1POAG6K6dL4aGYvApcDLxCEyjrgEbrwmnTB4gDhP3fCG+iERtI04FbgQjN7PS0ri49m1ogh6FmEqOBBXTZpzJB0MrDKzB7pti3jwPvN7D2EKegLJB2bFpbgfq0C7wH+1cwOB96gY2qkBD4CENdvnAr8pLNsovsY19+cRhCgewJT2XJ5wbjggqWdF4HZyfmsmFdWVkp6B0A8ruqyPduEpBpBrPzAzH4as0vlY0oMr98HHAPMiOFamNj37Z8Cp0paQpiSPZ6wDqIs/jWJ/7liZqsI6x6OpFz363JguZk9GM9vIQiYMvlYcBLwqJmtjOdl8vFE4HkzW21mg8BPCa/TcX9NumBp5yFg/7j6eRIhxLewyzZtTxYC58T0OcB/dNGWbSKuc1gAPG1m/5wUlcZHAEm7SZoR01OADxLW69wHfCxWm7B+mtkXzWyWme1NeP39yszOoiT+FUiaKmnnIg38BfAkJbpfzexlYJmkA2PWCcBTlMjHhDNpTQdBuXx8AThaUl98ny2ex3F/Tfov3XYg6cOEOfQKcJ2ZXdZlk8YESTcBxxG2BV8JXAzcBtwMzAGWAn9lZp0LcycEkt4PPAA8QWvtw5cI61hK4SOApD8hLHCrEP7huNnMLpW0LyEisSvwGPAJMxvonqXbjqTjgM+a2cll8y/687N4WgV+aGaXSZpJue7XuYTF05OA54Bzifct5fFxKuFDfV8zWxfzyvY8XgKcTvg25mPA+YQ1K+P6mnTB4jiO4zhOz+NTQo7jOI7j9DwuWBzHcRzH6XlcsDiO4ziO0/O4YHEcx3Ecp+dxweI4juM4Ts/jgsVxSo6kOyWds/WaI7bxZ5IWj5E9/yXp/LFoy3GcHQcXLI4zwZC0RNJGSRskrZR0Q9ySYEjM7CQzu3G48tFgZg+Y2YFbr7ntSDpA0k8kvSJpnaTfS/pM3E299MTn82vdtsNxeg0XLI4zMTnFzKYRfur8CODLnRUUmFCvcUnvIvzY3zLg3WY2Hfg4wcedu2mb4zjdZUK9mTmO007cj+ZO4FBoTrdcJul/gH5g33QKRtLfSPq1pMslrZH0vKSTivYk7SrpekkvxfLbYv5xkpYn9ZZI+qKkp2K96yXtFMt2kXS7pNWx7HZJs0bp0iXAb8zsM8Vut2a22Mz+Ou6dhKRTJS2StDb69scddv19jMq8IWmBpN3jtNh6SffEzdyQtLckkzQ/+rtC0meTtiZLujKWvRTTk9PxkHSRpFXx2nM7rr1c0gsxCvZvcSuFEa9V2LX5LOBzMYL281GOm+OUHhcsjjOBkTQb+DDhp7ELzgbmEyISS4e47ChgMWGbhm8BC+IeIQDfA/qAQ4C3A1eM0P1ZwIeAdwEH0IryZMD1wDsJP02+EbhqlC6dSNgkb0gkHUDYs+VCYDfgDuDnCnt/FfwlYY+lA4BTCILuS7F+Bnyyo9kPAPsT9vP5vKQTY/4/AEcDc4HDCJsTppGsPYDphJ8oPw+4uhBDwDdi/3OB/WKdr27tWjO7BvgB8C0zm2Zmpww3Fo6zo+GCxXEmJrdJWgv8Grgf+HpSdoOZLTKzetxdtZOlZvZdM2sQ9iV6B7C7wq6yJwF/a2ZrzGzQzO4fwYarzGxZ3CPlMsIGcJjZq2Z2q5n1m9n6WPbno/RrJrBihPLTgf80s7ujb5cDU4D3JXW+Y2YrY/TpAeBBM3vMzDYR9u85vKPNS8zsDTN7giC0zoz5ZwGXmtkqM1tNiP6cnVw3GMsHzewOYANwYBR/84FPm9lrcQy+TtjMccRrRzNAjrOjUt16FcdxepCPmtk9w5Qt28q1LxcJM+uPwZVphE3MXjOzNaO0Ie1nKbAngKQ+QmRmHlBEHHaWVIkiaSReJQio4diTJGpkZrmkZYRIRcHKJL1xiPPOBcqdfrx7qL5IfCxsNbN6ct4f296NEKV6pBW4QoQNK7d2reM4w+ARFscpH291R9NlwK6SZoyy/uwkPQd4KaYvIkQLjjKzPwKOjfli69xDmNIZjpcIU02hwaAIZgMvjtLmoRjOj7a+OspG4hWCMDrEzGbEx/S4SHo0+I60jjMELlgcxwEgLnK9E/iXuHC2JunYES65QNIsSbsS1nv8OObvTPjAXhvLLn4TZlwMvE/SP0naA0DSfpK+H4XUzcBHJJ0gqUYQRwPAb96Mrx18RVKfpEOAcxM/bgK+LGk3SW8jrEH5/tYaM7Mc+C5whaS3Rx/2kvShUdqzEtj3zTrhOGXHBYvjOClnE9ZX/AFYRVjcOhw/BO4CngOeBYrfDrmSsK7kFeC3wC9G27mZPQscA+wNLJK0DrgVeBhYb2aLgU8A34ntn0L4ivfm0fYxBPcDzwD3Apeb2V0x/2ux398DTwCP0vJxa3w+tvlbSa8TIkejXaOyADg4fgvqtlFe4zilR2YefXQc580haQlw/gjraHoeSXsDzwO1jvUkjuP0IB5hcRzHcRyn53HB4jiO4zhOz+NTQo7jOI7j9DweYXEcx3Ecp+dxweI4juM4Ts/jgsVxHMdxnJ7HBYvjOI7jOD2PCxbHcRzHcXqe/wfLh4nzANw9WgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 648x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the explained variance by each component as well as the cumulative one. The graph below shows that the \n",
    "# the first 40 components explain about 80% of the variance in the data. The marginal explanatory value of subsequent \n",
    "# components is relatively small. The last 15 components do not add any value in terms of explaining the variance in \n",
    "# the data. \n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "\n",
    "# Plot the explained variance.\n",
    "plt.plot(range(len(var_exp)), var_exp, lw=3, label = 'Variance Explained')\n",
    "\n",
    "# Plot the cumulative explained variance.\n",
    "plt.plot(range(len(var_exp)), cev, lw=3, color = 'orange', label = 'Cumulative Variance Explained')\n",
    "\n",
    "# Label the axes.\n",
    "plt.ylabel('Variance Explained', fontsize=12)\n",
    "plt.xlabel('Principal Component', fontsize=12)\n",
    "    \n",
    "# Add title and legend.\n",
    "plt.title('Component vs. Variance Explained', fontsize=16)\n",
    "plt.legend(fontsize=11);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the linear regression done earlier on 65 components. Instantiating: \n",
    "pca = PCA(n_components = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=65, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit PCA to training data.\n",
    "pca.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.1432\n",
      "Testing Score: 0.0872\n"
     ]
    }
   ],
   "source": [
    "# Instantiating linear regression model. These values are very similar to those of a regular linear regression. \n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Transforming Z_train and Z_test.\n",
    "Z_train = pca.transform(X_train_scaled)\n",
    "Z_test = pca.transform(X_test_transf)\n",
    "\n",
    "# Fitting on Z_train.\n",
    "linreg.fit(Z_train, y_train)\n",
    "\n",
    "# Scores on training and testing sets.\n",
    "print(f'Training Score: {round(linreg.score(Z_train, y_train),4)}')\n",
    "print(f'Testing Score: {round(linreg.score(Z_test, y_test),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the logistic regression to see if there is an improvement on scores. Recall: the logistic regression\n",
    "# was the best performer among multiclass regression models run in notebook 4. \n",
    "# Instantiating \n",
    "\n",
    "logreg = LogisticRegressionCV(multi_class='multinomial', solver = 'saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vvroseth/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/vvroseth/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/vvroseth/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Users/vvroseth/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.3043\n",
      "Testing Score: 0.2824\n"
     ]
    }
   ],
   "source": [
    "# Fitting on Z_train.\n",
    "logreg.fit(Z_train, y_train)\n",
    "\n",
    "# Scores on training and testing sets.\n",
    "print(f'Training Score: {round(logreg.score(Z_train, y_train),4)}')\n",
    "print(f'Testing Score: {round(logreg.score(Z_test, y_test),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The scores for the logistic and lineal regression are worse when using principal component analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PCA for Binary Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column to binarize \n",
    "colstep['binary_life_satisfaction'] = colstep['life_satisfaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarizing so that 0 = 5 or less, and 1 = 6 or more \n",
    "unhappy = [1,2,3,4,5]\n",
    "happy = [6,7,8,9,10]\n",
    "\n",
    "for number in unhappy: \n",
    "    colstep['binary_life_satisfaction'].replace(number,0, inplace=True)\n",
    "    \n",
    "for number in happy:\n",
    "    colstep['binary_life_satisfaction'].replace(number,1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    2276\n",
       "0.0     341\n",
       "Name: binary_life_satisfaction, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming new values of y\n",
    "colstep['binary_life_satisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting y for as a binary variable \n",
    "yb = colstep['binary_life_satisfaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the X values. Excluding life satisfaction and other features that have dummies. \n",
    "Xb = colstep.drop(columns=['life_satisfaction','binary_life_satisfaction', 'in_school', 'owns_house', 'house_beds',\n",
    "                          'house_kitchen','reported_social_status', 'got_pr_transf', \n",
    "                          'got_pu_transf', 'part_in_training','life_satisfaction', \n",
    "                          'offdays_ill', 'healthinsurance','speak_other_languaje', \n",
    "                          'lives_w_mother', 'lives_w_father','read_overall', \n",
    "                          'write_overall', 'numeracy_overall', 'supervise', \n",
    "                          'computer_use_overall', 'think_learn_work', 'autonomy_at_work',\n",
    "                          'repetitiveness_at_work', 'physical_demand_work', \n",
    "                          'has_children', 'hh_size', 'gender', 'has_spouse',\n",
    "                          'chronic_disease', 'shocks_bef_15', 'mother_tongue',\n",
    "                          'labor_market_status', 'job_stable', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test \n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, stratify=y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1962, 81)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making sure the shapes are identical \n",
    "Xb_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(655, 81)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1962,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(655,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.fit(Xb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming X train \n",
    "Xb_train_scaled = ss.transform(Xb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the array into a list so I can convert it into a dataframe and concatenate it to the dataframe with the unscaled features \n",
    "Xb_train_scaled = Xb_train_scaled.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the list into a dataframe \n",
    "Xb_train_scaled = pd.DataFrame(Xb_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.748998</td>\n",
       "      <td>0.487166</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>1.544888</td>\n",
       "      <td>0.653568</td>\n",
       "      <td>-2.077228</td>\n",
       "      <td>-2.126707</td>\n",
       "      <td>-1.026783</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>1.819914</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.346443</td>\n",
       "      <td>-1.019644</td>\n",
       "      <td>-1.261905</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>0.197720</td>\n",
       "      <td>1.385898</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>-0.600153</td>\n",
       "      <td>0.448477</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.201278</td>\n",
       "      <td>0.487166</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>1.565264</td>\n",
       "      <td>0.808710</td>\n",
       "      <td>-0.496361</td>\n",
       "      <td>1.532998</td>\n",
       "      <td>-0.369285</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>2.308470</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.748998</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>1.371286</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>-1.625672</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>0.590536</td>\n",
       "      <td>0.253108</td>\n",
       "      <td>3.719524</td>\n",
       "      <td>2.292723</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>3.246718</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.441884</td>\n",
       "      <td>-0.517374</td>\n",
       "      <td>0.712988</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>0.653568</td>\n",
       "      <td>-0.922853</td>\n",
       "      <td>0.590536</td>\n",
       "      <td>-2.306673</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>1.819914</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>-0.201278</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.653568</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>-0.496361</td>\n",
       "      <td>-1.026783</td>\n",
       "      <td>0.448477</td>\n",
       "      <td>2.292723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>0.346443</td>\n",
       "      <td>0.487166</td>\n",
       "      <td>-1.261905</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>-0.713976</td>\n",
       "      <td>-1.500041</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>-0.600153</td>\n",
       "      <td>0.448477</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>1.819914</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>1.441884</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>0.712988</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>-1.169824</td>\n",
       "      <td>0.808710</td>\n",
       "      <td>1.133985</td>\n",
       "      <td>1.106368</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>3.246718</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>0.894164</td>\n",
       "      <td>0.487166</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>0.653568</td>\n",
       "      <td>-1.500041</td>\n",
       "      <td>-0.496361</td>\n",
       "      <td>-1.453413</td>\n",
       "      <td>-0.369285</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>0.894164</td>\n",
       "      <td>-1.521914</td>\n",
       "      <td>0.712988</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>1.565264</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>1.677434</td>\n",
       "      <td>-1.026783</td>\n",
       "      <td>-0.369285</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1962 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.748998  0.487166 -0.603607  1.544888  0.653568 -2.077228 -2.126707   \n",
       "1     0.346443 -1.019644 -1.261905 -0.375131  0.197720  1.385898  0.047088   \n",
       "2    -0.201278  0.487166  0.054690 -0.375131  1.565264  0.808710 -0.496361   \n",
       "3    -0.748998 -0.015104  1.371286  0.264875 -1.625672 -0.345665  0.590536   \n",
       "4     1.441884 -0.517374  0.712988 -0.375131  0.653568 -0.922853  0.590536   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1957 -0.201278  0.989436 -0.603607  0.264875  0.653568 -0.345665 -0.496361   \n",
       "1958  0.346443  0.487166 -1.261905  0.264875 -0.713976 -1.500041  0.047088   \n",
       "1959  1.441884 -0.015104  0.712988  0.264875 -1.169824  0.808710  1.133985   \n",
       "1960  0.894164  0.487166  0.054690 -0.375131  0.653568 -1.500041 -0.496361   \n",
       "1961  0.894164 -1.521914  0.712988 -0.375131  1.565264 -0.345665  1.677434   \n",
       "\n",
       "            7         8         9   ...        71        72        73  \\\n",
       "0    -1.026783 -1.187047 -0.563959  ... -1.402936 -0.787495  1.819914   \n",
       "1    -0.600153  0.448477 -0.563959  ...  0.712791  1.269849 -0.549476   \n",
       "2     1.532998 -0.369285 -0.563959  ... -1.402936 -0.787495 -0.549476   \n",
       "3     0.253108  3.719524  2.292723  ... -1.402936 -0.787495 -0.549476   \n",
       "4    -2.306673 -1.187047 -0.563959  ...  0.712791  1.269849  1.819914   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1957 -1.026783  0.448477  2.292723  ...  0.712791  1.269849 -0.549476   \n",
       "1958 -0.600153  0.448477 -0.563959  ... -1.402936 -0.787495  1.819914   \n",
       "1959  1.106368 -1.187047 -0.563959  ... -1.402936 -0.787495 -0.549476   \n",
       "1960 -1.453413 -0.369285 -0.563959  ... -1.402936 -0.787495 -0.549476   \n",
       "1961 -1.026783 -0.369285 -0.563959  ... -1.402936 -0.787495 -0.549476   \n",
       "\n",
       "            74        75        76        77        78   79        80  \n",
       "0    -0.308003 -0.742286 -0.433187 -0.283543 -0.075088  0.0  1.504845  \n",
       "1    -0.308003  1.347190 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "2    -0.308003 -0.742286  2.308470 -0.283543 -0.075088  0.0 -0.664520  \n",
       "3     3.246718 -0.742286 -0.433187 -0.283543 -0.075088  0.0  1.504845  \n",
       "4    -0.308003 -0.742286 -0.433187 -0.283543 -0.075088  0.0  1.504845  \n",
       "...        ...       ...       ...       ...       ...  ...       ...  \n",
       "1957 -0.308003 -0.742286 -0.433187 -0.283543 -0.075088  0.0  1.504845  \n",
       "1958 -0.308003 -0.742286 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "1959  3.246718 -0.742286 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "1960 -0.308003  1.347190 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "1961 -0.308003  1.347190 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "\n",
       "[1962 rows x 81 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the resulting dataframe \n",
    "Xb_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns \n",
    "Xb_train_scaled.rename(columns = scalednames, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ses_15</th>\n",
       "      <th>extraversion_av</th>\n",
       "      <th>conscientiousness_avg</th>\n",
       "      <th>openness_av</th>\n",
       "      <th>stability_av</th>\n",
       "      <th>agreeableness_av</th>\n",
       "      <th>grit_av</th>\n",
       "      <th>decision_av</th>\n",
       "      <th>hostile_av</th>\n",
       "      <th>risk</th>\n",
       "      <th>...</th>\n",
       "      <th>labor_market_status_1.0</th>\n",
       "      <th>job_stable_1.0</th>\n",
       "      <th>highest_ISCED_PIAAC_1</th>\n",
       "      <th>highest_ISCED_PIAAC_2</th>\n",
       "      <th>highest_ISCED_PIAAC_3</th>\n",
       "      <th>highest_ISCED_PIAAC_5</th>\n",
       "      <th>highest_ISCED_PIAAC_6</th>\n",
       "      <th>highest_ISCED_PIAAC_7</th>\n",
       "      <th>highest_ISCED_PIAAC_8</th>\n",
       "      <th>dropout_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.748998</td>\n",
       "      <td>0.487166</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>1.544888</td>\n",
       "      <td>0.653568</td>\n",
       "      <td>-2.077228</td>\n",
       "      <td>-2.126707</td>\n",
       "      <td>-1.026783</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>1.819914</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.346443</td>\n",
       "      <td>-1.019644</td>\n",
       "      <td>-1.261905</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>0.197720</td>\n",
       "      <td>1.385898</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>-0.600153</td>\n",
       "      <td>0.448477</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.201278</td>\n",
       "      <td>0.487166</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>1.565264</td>\n",
       "      <td>0.808710</td>\n",
       "      <td>-0.496361</td>\n",
       "      <td>1.532998</td>\n",
       "      <td>-0.369285</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>2.308470</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.748998</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>1.371286</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>-1.625672</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>0.590536</td>\n",
       "      <td>0.253108</td>\n",
       "      <td>3.719524</td>\n",
       "      <td>2.292723</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>3.246718</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.441884</td>\n",
       "      <td>-0.517374</td>\n",
       "      <td>0.712988</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>0.653568</td>\n",
       "      <td>-0.922853</td>\n",
       "      <td>0.590536</td>\n",
       "      <td>-2.306673</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>1.819914</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ses_15  extraversion_av  conscientiousness_avg  openness_av  \\\n",
       "0 -0.748998         0.487166              -0.603607     1.544888   \n",
       "1  0.346443        -1.019644              -1.261905    -0.375131   \n",
       "2 -0.201278         0.487166               0.054690    -0.375131   \n",
       "3 -0.748998        -0.015104               1.371286     0.264875   \n",
       "4  1.441884        -0.517374               0.712988    -0.375131   \n",
       "\n",
       "   stability_av  agreeableness_av   grit_av  decision_av  hostile_av  \\\n",
       "0      0.653568         -2.077228 -2.126707    -1.026783   -1.187047   \n",
       "1      0.197720          1.385898  0.047088    -0.600153    0.448477   \n",
       "2      1.565264          0.808710 -0.496361     1.532998   -0.369285   \n",
       "3     -1.625672         -0.345665  0.590536     0.253108    3.719524   \n",
       "4      0.653568         -0.922853  0.590536    -2.306673   -1.187047   \n",
       "\n",
       "       risk  ...  labor_market_status_1.0  job_stable_1.0  \\\n",
       "0 -0.563959  ...                -1.402936       -0.787495   \n",
       "1 -0.563959  ...                 0.712791        1.269849   \n",
       "2 -0.563959  ...                -1.402936       -0.787495   \n",
       "3  2.292723  ...                -1.402936       -0.787495   \n",
       "4 -0.563959  ...                 0.712791        1.269849   \n",
       "\n",
       "   highest_ISCED_PIAAC_1  highest_ISCED_PIAAC_2  highest_ISCED_PIAAC_3  \\\n",
       "0               1.819914              -0.308003              -0.742286   \n",
       "1              -0.549476              -0.308003               1.347190   \n",
       "2              -0.549476              -0.308003              -0.742286   \n",
       "3              -0.549476               3.246718              -0.742286   \n",
       "4               1.819914              -0.308003              -0.742286   \n",
       "\n",
       "   highest_ISCED_PIAAC_5  highest_ISCED_PIAAC_6  highest_ISCED_PIAAC_7  \\\n",
       "0              -0.433187              -0.283543              -0.075088   \n",
       "1              -0.433187              -0.283543              -0.075088   \n",
       "2               2.308470              -0.283543              -0.075088   \n",
       "3              -0.433187              -0.283543              -0.075088   \n",
       "4              -0.433187              -0.283543              -0.075088   \n",
       "\n",
       "   highest_ISCED_PIAAC_8  dropout_1.0  \n",
       "0                    0.0     1.504845  \n",
       "1                    0.0    -0.664520  \n",
       "2                    0.0    -0.664520  \n",
       "3                    0.0     1.504845  \n",
       "4                    0.0     1.504845  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming \n",
    "Xb_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1962, 81)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming that the shape is intact\n",
    "Xb_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ses_15</th>\n",
       "      <th>extraversion_av</th>\n",
       "      <th>conscientiousness_avg</th>\n",
       "      <th>openness_av</th>\n",
       "      <th>stability_av</th>\n",
       "      <th>agreeableness_av</th>\n",
       "      <th>grit_av</th>\n",
       "      <th>decision_av</th>\n",
       "      <th>hostile_av</th>\n",
       "      <th>risk</th>\n",
       "      <th>...</th>\n",
       "      <th>labor_market_status_1.0</th>\n",
       "      <th>job_stable_1.0</th>\n",
       "      <th>highest_ISCED_PIAAC_1</th>\n",
       "      <th>highest_ISCED_PIAAC_2</th>\n",
       "      <th>highest_ISCED_PIAAC_3</th>\n",
       "      <th>highest_ISCED_PIAAC_5</th>\n",
       "      <th>highest_ISCED_PIAAC_6</th>\n",
       "      <th>highest_ISCED_PIAAC_7</th>\n",
       "      <th>highest_ISCED_PIAAC_8</th>\n",
       "      <th>dropout_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ses_15  extraversion_av  conscientiousness_avg  openness_av  \\\n",
       "43      4.0         3.000000               3.333333     3.666667   \n",
       "311     6.0         2.666667               3.000000     3.333333   \n",
       "294     8.0         3.666667               3.666667     4.000000   \n",
       "164     3.0         2.000000               3.333333     1.666667   \n",
       "720     3.0         2.333333               3.000000     3.333333   \n",
       "\n",
       "     stability_av  agreeableness_av   grit_av  decision_av  hostile_av  risk  \\\n",
       "43       1.666667          3.666667  3.666667         3.75         1.0   1.0   \n",
       "311      1.666667          3.333333  3.666667         3.75         1.0   1.0   \n",
       "294      3.333333          3.333333  2.666667         4.00         1.0   2.0   \n",
       "164      2.666667          2.333333  3.000000         2.25         1.5   2.0   \n",
       "720      3.666667          3.000000  3.333333         3.25         1.0   1.0   \n",
       "\n",
       "     ...  labor_market_status_1.0  job_stable_1.0  highest_ISCED_PIAAC_1  \\\n",
       "43   ...                        0               0                      1   \n",
       "311  ...                        1               1                      0   \n",
       "294  ...                        0               0                      0   \n",
       "164  ...                        1               1                      0   \n",
       "720  ...                        1               1                      0   \n",
       "\n",
       "     highest_ISCED_PIAAC_2  highest_ISCED_PIAAC_3  highest_ISCED_PIAAC_5  \\\n",
       "43                       0                      0                      0   \n",
       "311                      0                      0                      1   \n",
       "294                      0                      1                      0   \n",
       "164                      1                      0                      0   \n",
       "720                      0                      1                      0   \n",
       "\n",
       "     highest_ISCED_PIAAC_6  highest_ISCED_PIAAC_7  highest_ISCED_PIAAC_8  \\\n",
       "43                       0                      0                      0   \n",
       "311                      0                      0                      0   \n",
       "294                      0                      0                      0   \n",
       "164                      0                      0                      0   \n",
       "720                      0                      0                      0   \n",
       "\n",
       "     dropout_1.0  \n",
       "43             0  \n",
       "311            1  \n",
       "294            0  \n",
       "164            1  \n",
       "720            0  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming the X test data \n",
    "Xb_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(655, 81)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of data \n",
    "Xb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming X test \n",
    "Xb_test_transf = ss.transform(Xb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20127757, -0.01510394,  0.0546902 , ..., -0.07508751,\n",
       "         0.        , -0.66452018],\n",
       "       [ 0.89416374, -0.5173737 , -0.60360723, ..., -0.07508751,\n",
       "         0.        ,  1.5048452 ],\n",
       "       [ 1.98960506,  0.98943595,  0.7129881 , ..., -0.07508751,\n",
       "         0.        , -0.66452018],\n",
       "       ...,\n",
       "       [-0.20127757,  0.98943595, -0.60360723, ..., -0.07508751,\n",
       "         0.        ,  1.5048452 ],\n",
       "       [-0.20127757,  0.98943595,  0.7129881 , ..., -0.07508751,\n",
       "         0.        , -0.66452018],\n",
       "       [ 0.89416374,  1.49170572,  0.7129881 , ..., -0.07508751,\n",
       "         0.        , -0.66452018]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the result\n",
    "Xb_test_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the array into a list so I can convert it into a dataframe and concatenate it to the dataframe with the unscaled features \n",
    "Xb_test_transf = Xb_test_transf.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the list into a dataframe \n",
    "Xb_test_transf = pd.DataFrame(Xb_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.201278</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>0.904882</td>\n",
       "      <td>-1.169824</td>\n",
       "      <td>0.808710</td>\n",
       "      <td>1.133985</td>\n",
       "      <td>1.106368</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>1.819914</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.894164</td>\n",
       "      <td>-0.517374</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>-1.169824</td>\n",
       "      <td>0.231522</td>\n",
       "      <td>1.133985</td>\n",
       "      <td>1.106368</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>2.308470</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.989605</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>0.712988</td>\n",
       "      <td>1.544888</td>\n",
       "      <td>1.109416</td>\n",
       "      <td>0.231522</td>\n",
       "      <td>-0.496361</td>\n",
       "      <td>1.532998</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.748998</td>\n",
       "      <td>-1.521914</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>-2.935157</td>\n",
       "      <td>0.197720</td>\n",
       "      <td>-1.500041</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>-1.453413</td>\n",
       "      <td>-0.369285</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>3.246718</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.748998</td>\n",
       "      <td>-1.019644</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>1.565264</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>0.590536</td>\n",
       "      <td>0.253108</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>0.346443</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>1.109416</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>-0.173522</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>2.308470</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>0.894164</td>\n",
       "      <td>1.491706</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>1.544888</td>\n",
       "      <td>-0.258128</td>\n",
       "      <td>-0.922853</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>1.532998</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>1.819914</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>-0.201278</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>-0.375131</td>\n",
       "      <td>0.197720</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>1.133985</td>\n",
       "      <td>0.253108</td>\n",
       "      <td>1.266239</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>2.308470</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>-0.201278</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>0.712988</td>\n",
       "      <td>1.544888</td>\n",
       "      <td>0.197720</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>-0.173522</td>\n",
       "      <td>1.266239</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>0.894164</td>\n",
       "      <td>1.491706</td>\n",
       "      <td>0.712988</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>0.653568</td>\n",
       "      <td>1.385898</td>\n",
       "      <td>1.677434</td>\n",
       "      <td>1.106368</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>2.308470</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>655 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.201278 -0.015104  0.054690  0.904882 -1.169824  0.808710  1.133985   \n",
       "1    0.894164 -0.517374 -0.603607  0.264875 -1.169824  0.231522  1.133985   \n",
       "2    1.989605  0.989436  0.712988  1.544888  1.109416  0.231522 -0.496361   \n",
       "3   -0.748998 -1.521914  0.054690 -2.935157  0.197720 -1.500041  0.047088   \n",
       "4   -0.748998 -1.019644 -0.603607  0.264875  1.565264 -0.345665  0.590536   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "650  0.346443  0.989436 -0.603607  0.264875  1.109416 -0.345665  0.047088   \n",
       "651  0.894164  1.491706 -0.603607  1.544888 -0.258128 -0.922853  0.047088   \n",
       "652 -0.201278  0.989436 -0.603607 -0.375131  0.197720 -0.345665  1.133985   \n",
       "653 -0.201278  0.989436  0.712988  1.544888  0.197720 -0.345665  0.047088   \n",
       "654  0.894164  1.491706  0.712988  0.264875  0.653568  1.385898  1.677434   \n",
       "\n",
       "           7         8         9   ...        71        72        73  \\\n",
       "0    1.106368 -1.187047 -0.563959  ... -1.402936 -0.787495  1.819914   \n",
       "1    1.106368 -1.187047 -0.563959  ...  0.712791  1.269849 -0.549476   \n",
       "2    1.532998 -1.187047  0.388268  ... -1.402936 -0.787495 -0.549476   \n",
       "3   -1.453413 -0.369285  0.388268  ...  0.712791  1.269849 -0.549476   \n",
       "4    0.253108 -1.187047 -0.563959  ...  0.712791  1.269849 -0.549476   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "650 -0.173522 -1.187047 -0.563959  ...  0.712791 -0.787495 -0.549476   \n",
       "651  1.532998 -1.187047 -0.563959  ... -1.402936 -0.787495  1.819914   \n",
       "652  0.253108  1.266239 -0.563959  ...  0.712791 -0.787495 -0.549476   \n",
       "653 -0.173522  1.266239 -0.563959  ... -1.402936 -0.787495 -0.549476   \n",
       "654  1.106368 -1.187047 -0.563959  ...  0.712791  1.269849 -0.549476   \n",
       "\n",
       "           74        75        76        77        78   79        80  \n",
       "0   -0.308003 -0.742286 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "1   -0.308003 -0.742286  2.308470 -0.283543 -0.075088  0.0  1.504845  \n",
       "2   -0.308003  1.347190 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "3    3.246718 -0.742286 -0.433187 -0.283543 -0.075088  0.0  1.504845  \n",
       "4   -0.308003  1.347190 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "..        ...       ...       ...       ...       ...  ...       ...  \n",
       "650 -0.308003 -0.742286  2.308470 -0.283543 -0.075088  0.0 -0.664520  \n",
       "651 -0.308003 -0.742286 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "652 -0.308003 -0.742286  2.308470 -0.283543 -0.075088  0.0  1.504845  \n",
       "653 -0.308003  1.347190 -0.433187 -0.283543 -0.075088  0.0 -0.664520  \n",
       "654 -0.308003 -0.742286  2.308470 -0.283543 -0.075088  0.0 -0.664520  \n",
       "\n",
       "[655 rows x 81 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the resulting dataframe \n",
    "Xb_test_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(655, 81)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of standardized test data \n",
    "Xb_test_transf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns \n",
    "Xb_test_transf.rename(columns = scalednames, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ses_15</th>\n",
       "      <th>extraversion_av</th>\n",
       "      <th>conscientiousness_avg</th>\n",
       "      <th>openness_av</th>\n",
       "      <th>stability_av</th>\n",
       "      <th>agreeableness_av</th>\n",
       "      <th>grit_av</th>\n",
       "      <th>decision_av</th>\n",
       "      <th>hostile_av</th>\n",
       "      <th>risk</th>\n",
       "      <th>...</th>\n",
       "      <th>labor_market_status_1.0</th>\n",
       "      <th>job_stable_1.0</th>\n",
       "      <th>highest_ISCED_PIAAC_1</th>\n",
       "      <th>highest_ISCED_PIAAC_2</th>\n",
       "      <th>highest_ISCED_PIAAC_3</th>\n",
       "      <th>highest_ISCED_PIAAC_5</th>\n",
       "      <th>highest_ISCED_PIAAC_6</th>\n",
       "      <th>highest_ISCED_PIAAC_7</th>\n",
       "      <th>highest_ISCED_PIAAC_8</th>\n",
       "      <th>dropout_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.201278</td>\n",
       "      <td>-0.015104</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>0.904882</td>\n",
       "      <td>-1.169824</td>\n",
       "      <td>0.808710</td>\n",
       "      <td>1.133985</td>\n",
       "      <td>1.106368</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>1.819914</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.894164</td>\n",
       "      <td>-0.517374</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>-1.169824</td>\n",
       "      <td>0.231522</td>\n",
       "      <td>1.133985</td>\n",
       "      <td>1.106368</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>2.308470</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.989605</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>0.712988</td>\n",
       "      <td>1.544888</td>\n",
       "      <td>1.109416</td>\n",
       "      <td>0.231522</td>\n",
       "      <td>-0.496361</td>\n",
       "      <td>1.532998</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.402936</td>\n",
       "      <td>-0.787495</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.748998</td>\n",
       "      <td>-1.521914</td>\n",
       "      <td>0.054690</td>\n",
       "      <td>-2.935157</td>\n",
       "      <td>0.197720</td>\n",
       "      <td>-1.500041</td>\n",
       "      <td>0.047088</td>\n",
       "      <td>-1.453413</td>\n",
       "      <td>-0.369285</td>\n",
       "      <td>0.388268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>3.246718</td>\n",
       "      <td>-0.742286</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.748998</td>\n",
       "      <td>-1.019644</td>\n",
       "      <td>-0.603607</td>\n",
       "      <td>0.264875</td>\n",
       "      <td>1.565264</td>\n",
       "      <td>-0.345665</td>\n",
       "      <td>0.590536</td>\n",
       "      <td>0.253108</td>\n",
       "      <td>-1.187047</td>\n",
       "      <td>-0.563959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>1.269849</td>\n",
       "      <td>-0.549476</td>\n",
       "      <td>-0.308003</td>\n",
       "      <td>1.347190</td>\n",
       "      <td>-0.433187</td>\n",
       "      <td>-0.283543</td>\n",
       "      <td>-0.075088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.664520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ses_15  extraversion_av  conscientiousness_avg  openness_av  \\\n",
       "0 -0.201278        -0.015104               0.054690     0.904882   \n",
       "1  0.894164        -0.517374              -0.603607     0.264875   \n",
       "2  1.989605         0.989436               0.712988     1.544888   \n",
       "3 -0.748998        -1.521914               0.054690    -2.935157   \n",
       "4 -0.748998        -1.019644              -0.603607     0.264875   \n",
       "\n",
       "   stability_av  agreeableness_av   grit_av  decision_av  hostile_av  \\\n",
       "0     -1.169824          0.808710  1.133985     1.106368   -1.187047   \n",
       "1     -1.169824          0.231522  1.133985     1.106368   -1.187047   \n",
       "2      1.109416          0.231522 -0.496361     1.532998   -1.187047   \n",
       "3      0.197720         -1.500041  0.047088    -1.453413   -0.369285   \n",
       "4      1.565264         -0.345665  0.590536     0.253108   -1.187047   \n",
       "\n",
       "       risk  ...  labor_market_status_1.0  job_stable_1.0  \\\n",
       "0 -0.563959  ...                -1.402936       -0.787495   \n",
       "1 -0.563959  ...                 0.712791        1.269849   \n",
       "2  0.388268  ...                -1.402936       -0.787495   \n",
       "3  0.388268  ...                 0.712791        1.269849   \n",
       "4 -0.563959  ...                 0.712791        1.269849   \n",
       "\n",
       "   highest_ISCED_PIAAC_1  highest_ISCED_PIAAC_2  highest_ISCED_PIAAC_3  \\\n",
       "0               1.819914              -0.308003              -0.742286   \n",
       "1              -0.549476              -0.308003              -0.742286   \n",
       "2              -0.549476              -0.308003               1.347190   \n",
       "3              -0.549476               3.246718              -0.742286   \n",
       "4              -0.549476              -0.308003               1.347190   \n",
       "\n",
       "   highest_ISCED_PIAAC_5  highest_ISCED_PIAAC_6  highest_ISCED_PIAAC_7  \\\n",
       "0              -0.433187              -0.283543              -0.075088   \n",
       "1               2.308470              -0.283543              -0.075088   \n",
       "2              -0.433187              -0.283543              -0.075088   \n",
       "3              -0.433187              -0.283543              -0.075088   \n",
       "4              -0.433187              -0.283543              -0.075088   \n",
       "\n",
       "   highest_ISCED_PIAAC_8  dropout_1.0  \n",
       "0                    0.0    -0.664520  \n",
       "1                    0.0     1.504845  \n",
       "2                    0.0    -0.664520  \n",
       "3                    0.0     1.504845  \n",
       "4                    0.0    -0.664520  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb_test_transf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a binary logistic regression  \n",
    "logregbin = LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming Z_train and Z_test.\n",
    "Zb_train = pca.transform(Xb_train_scaled)\n",
    "Zb_test = pca.transform(Xb_test_transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vvroseth/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8695\n",
      "Testing Score: 0.8702\n"
     ]
    }
   ],
   "source": [
    "# Fitting on Z_train.\n",
    "logregbin.fit(Zb_train, yb_train)\n",
    "\n",
    "# Scores on training and testing sets.\n",
    "print(f'Training Score: {round(logregbin.score(Zb_train, yb_train),4)}')\n",
    "print(f'Testing Score: {round(logregbin.score(Zb_test, yb_test),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The scores for the binary classification using PCA are identical to those run in notebook 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
